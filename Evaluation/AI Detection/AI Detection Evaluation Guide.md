# AI Detection Model Evaluation Guide

**Contributors:** KhushiChoubey, Wang

## 1. Labelling System

In AI detection models, the model uses three classes for labeling text according to its origin:

### 1. AI – Text that is entirely generated by an AI model
- **Key characteristics:** Repetitive patterns, overly formal language, lack of personal anecdotes or natural imperfections.

### 2. Human – Text that is entirely written by a human
- **Key characteristics:** Personal anecdotes, natural flow, minor errors or imperfections, contextually nuanced writing.

### 3. Hybrid – Text created through a combination of human writing and AI assistance
- **Key characteristics:** Mixture of AI-like formal structure and human-like personal touches or edits; AI-generated sections edited or augmented by humans.

## 2. Rating Criteria

### 1. Repetition
AI-generated text often exhibits unnatural or excessive repetition of phrases and structures due to the patterns learned from training data. In contrast, human-written text is usually less repetitive unless repetition is intentional, such as for emphasis or storytelling purposes. Evaluating repetition helps determine whether the writing exhibits AI-like redundancy or natural human variation.

### 2. Lexical Diversity
AI text may recycle common vocabulary and avoid rare or unexpected word choices, resulting in a more uniform lexicon. Human writers naturally mix simple and advanced words and may include domain-specific terms or personal slang. Lexical diversity indicates whether the text demonstrates human creativity or AI-generated predictability.

### 3. Sentence Structure Diversity
AI tends to maintain uniform sentence length and complexity, producing a mechanical rhythm. Human writing, however, alternates naturally between short, long, fragmented, and casual sentence structures. Assessing sentence structure diversity helps identify whether the flow of the text is human-like or AI-generated.

### 4. Grammar
AI-generated text usually contains few to no grammar or spelling mistakes, and when errors occur, they are systematic, such as awkward phrasing or factual inaccuracies. Human text often includes typos, casual punctuation, and occasional unpolished grammar. Observing grammar patterns can indicate the likely authorship.

### 5. Content Specificity
AI tends to produce generic, templated, or universal responses that lack grounding in real-life details. Human writers refer to specific experiences, contexts, places, or unique references. Examining content specificity can help determine whether the text is personally grounded or broadly generic.

### 6. Emotional Expressiveness
AI-generated text usually remains neutral, polite, or overly formal, with emotions expressed in generic or exaggerated "safe" ways. Human writing is naturally subjective, showing nuanced emotions such as sarcasm, frustration, excitement, or subtle humor. Emotional expressiveness is a key marker of human authorship.

### 7. Coherence & Natural Transitions
AI text is often overly structured and coherent, sometimes to the point of rigidity. Human writing is more flexible and may include digressions, tangents, or abrupt shifts. Evaluating coherence and transitions helps identify whether the flow feels natural or mechanically constructed.

### 8. Pronouns
AI tends to avoid strong personal pronouns like "I" or "me" unless specifically instructed, maintaining a detached tone. Human authors freely use pronouns such as "I," "we," or "you" to situate writing within a lived perspective. Pronoun usage can indicate whether the text has personal, human elements.

### 9. Contextual Appropriateness
AI may apply an overly formal or academic tone even in casual contexts and can struggle with hyper-local or niche references. Human writing naturally adjusts tone depending on the audience, situation, and familiarity. Assessing contextual appropriateness helps determine whether the text is human-authored or AI-generated.

## 3. Human Rating Method

For human rating method, evaluating all of the samples contained in the testing dataset might lead to high complexity and nonsense. Instead, there are a few samples that should be taken into consideration. The evaluation should follow some set of criteria.

### a. Linguistic Patterns to Consider

#### AI-Generated Text Indicators
- Unnatural repetition of phrases or sentence structures
- Overly consistent sentence length and complexity
- Lack of personal pronouns or first-person experiences
- Generic or template-like responses
- Perfect grammar and punctuation (too polished)
- Absence of colloquialisms or informal language
- Repetitive use of transition words
- Overly formal or academic tone in casual contexts
- Lack of personal voice or unique perspective
- Overly comprehensive coverage without depth

#### Human-Written Text Indicators
- Natural flow and rhythm in writing
- Personal anecdotes or experiences
- Imperfect grammar or punctuation
- Use of colloquialisms and informal language
- Varied sentence structures and lengths
- Emotional or subjective language
- Context-specific references
- Natural transitions between ideas
- Personal voice and unique perspective
- Inconsistent depth of coverage

#### Hybrid Text Indicators
- Mix of formal and informal language patterns
- Some sections with perfect grammar, others with natural imperfections
- Combination of generic and personal content
- AI-generated structure with human personalization
- Human-written content with AI-enhanced vocabulary
- Inconsistent writing style throughout the text
- Some sections lack personal voice, others are highly personal
- Mix of template-like and original content
- AI-generated facts with human opinions or experiences
- Professional tone with personal touches

### b. Rating Process

The evaluation team suggests the evaluation should follow these processes:

#### Step 1: Initial Reading & Prediction Result
- Read the text completely without making immediate judgments.
- Note the overall tone, style, and content, as well as any obvious patterns or characteristics.
- Determine the model's prediction result:
  - **Correct Prediction:** The model's label matches the actual authorship (AI, Human, or Hybrid).
  - **Wrong Prediction:** The model's label does not match the actual authorship.

#### Step 2: Pattern Analysis
- Examine the text for linguistic patterns, sentence structures, grammar, and style.
- Evaluate whether the text aligns with typical human or AI writing patterns.
- Identify sections that may show hybrid authorship (both human and AI contributions).

#### Step 3: Justification Based on Rating Criteria
- Use established criteria (Repetition, Lexical Diversity, Sentence Structure Diversity, Grammar, Content Specificity, Emotional Expressiveness, Coherence & Natural Transitions, Pronouns, Contextual Appropriateness).
- Justify why the prediction is correct or wrong according to these criteria.
- Highlight specific examples from the text that support your reasoning.

#### Step 4: Confidence Rating
- Rate your confidence in the classification on a scale from 1 to 5 (1 = low confidence, 5 = high confidence).
- Document any ambiguous or borderline cases for team discussion.
- Explain why the confidence level was chosen, especially for hybrid or mixed-author texts.

### c. Example

**Text:** "I recently tried making homemade pasta for the first time. It was a bit messy, but surprisingly fun! The dough felt sticky at first, but after some kneading, it turned smooth. I think next time I'll try adding spinach for color."

**Prediction Result:** Human  
**Label:** Human

**Pattern Analysis:**
- **Repetition:** Minimal repetition; no mechanical patterns. (Human-like)
- **Lexical Diversity:** Uses simple but context-appropriate vocabulary ("sticky," "kneading," "smooth").
- **Sentence Structure Diversity:** Mix of short and longer sentences.
- **Grammar:** Minor casual imperfections possible; overall natural.
- **Content Specificity:** Mentions a personal cooking experience with specific details.
- **Emotional Expressiveness:** Shows excitement and humor ("surprisingly fun!").
- **Coherence & Natural Transitions:** Smooth progression from attempt → challenge → reflection → plan for next time.
- **Pronouns:** Uses "I" to situate the narrative personally.
- **Contextual Appropriateness:** Casual, informal tone fits the subject matter.

## 4. GenAI Rating

### a. Base Prompt

You are tasked with assessing whether an AI-detection model made a Correct or Wrong prediction when classifying text as AI or Human. Follow the criteria below strictly and provide reasoning based on each criterion.

**Evaluation Criteria:**

1. **Repetition**
   - AI: Often shows unnatural or excessive repetition of phrases or structures due to training patterns.
   - Human: Usually less repetitive, unless repetition is intentional (e.g., emphasis, storytelling).

2. **Lexical Diversity**
   - AI: May recycle common vocabulary, avoid rare or unexpected word choices.
   - Human: Mixes simple, advanced, and sometimes domain-specific or personal slang words naturally.

3. **Sentence Structure Diversity**
   - AI: Tends to use uniform sentence lengths and complexity (mechanical rhythm).
   - Human: Naturally alternates between short, long, fragmented, and casual structures.

4. **Grammar**
   - AI: Few to no grammar/spelling mistakes; errors are rare but often systematic.
   - Human: Casual typos, informal punctuation, and occasional unpolished grammar appear naturally.

5. **Content Specificity**
   - AI: Generic, templated, or universal responses lacking grounding in real-life details.
   - Human: References specific experiences, contexts, places, or unique details.

6. **Emotional Expressiveness**
   - AI: Neutral, polite, or overly formal; emotions are generic or exaggerated in "safe" ways.
   - Human: Subjective, nuanced, may include sarcasm, frustration, excitement, etc.

7. **Coherence & Natural Transitions**
   - AI: Overly structured, sometimes rigid.
   - Human: Flexible, may include digressions, tangents, or abrupt shifts.

8. **Pronouns**
   - AI: Avoids strong personal pronouns ("I", "me") unless instructed; keeps a detached tone.
   - Human: Uses "I", "we", or "you" freely to situate writing in a lived perspective.

9. **Contextual Appropriateness**
   - AI: Overly formal or academic even in casual contexts; struggles with hyper-local or niche references.
   - Human: Adjusts tone naturally depending on audience, situation, and context.

**Evaluation Output Format:**

For each text, provide:
- **Result:** Correct or Wrong (compare the model prediction with the ground truth label)
- **Criteria-based Analysis:** Explain which criteria indicate AI or Human characteristics, with examples from the text.
- **Confidence Level:** from 1 to 5

**Example:**

**Text:** "I started running every morning last month. At first, it was exhausting, but now I feel more energetic and even sleep better. My neighbor John sometimes joins me, and it makes it more fun."

**Label:** Human  
**Prediction:** AI  
**Result:** Wrong

**Criteria-based Analysis:**
- **Repetition:** No unnatural repetition → Human-like
- **Lexical Diversity:** Mix of common and specific words ("exhausting", "energetic", "neighbor John") → Human
- **Sentence Structure Diversity:** Natural variation → Human
- **Grammar:** Casual but correct → Human
- **Content Specificity:** Mentions timeframe, names, routine → Human
- **Emotional Expressiveness:** Subjective emotions → Human
- **Coherence & Transitions:** Conversational flow → Human
- **Pronouns:** Frequent personal pronouns ("I", "my") → Human
- **Contextual Appropriateness:** Fits casual context → Human

**Confidence Level:** 4.5

Then give the detection result with:
- Text: {Text}
- Label: {Label}
- Prediction: {Prediction}

### b. Using ChatGPT

Passing the base prompt with the modification of the {text}, {label} and {prediction}. Then ChatGPT will return the result with analysis and confidence level. However, this will be time-consuming since we might need to manually pass to ChatGPT.

### c. Using Evaluation Pipeline with GenAI

To achieve a robust and convenient evaluation method, the evaluation team has updated the EvaluateModel pipeline to include two functions for different purposes: one for evaluating Hugging Face models and another for evaluating commercial models.

#### i. Evaluating Hugging Face Models

The pipeline first processes the evaluation using a results pipeline containing text, labels, and prediction. The outputs can be saved as CSV or pickle files. It then loads the processed data and runs the evaluation through the model_based_evaluation function, which assesses the model and prints the results.

```python
from huggingface_hub import login
model_id = "mistralai/Mistral-7B-Instruct-v0.2"
token = "YOUR_TOKEN"
login(token=token)

model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto"
    )
tokenizer = AutoTokenizer.from_pretrained(model_id)
testset = pd.read_csv("/kaggle/input/test-feedback/Code Review Feedback Examples.csv")
test_val = EvaluateModel(model=model, tokenizer=tokenizer, dataset = testset, model_type="ai_detection",device = "automap") #feedback_generation/ai_detection 
test_val.model_based_evaluation()
```

In this example, we load the Mistral-7B model with its corresponding tokenizer and pass them into EvaluateModel. The evaluation is conducted on our test dataset (testset) for AI detection, so model_type is set to "ai_detection". The device parameter can be set to "automap" to automatically select the appropriate device. Running model_based_evaluation() then performs the evaluation and returns a detailed analysis.

#### ii. Evaluating Commercial Models

The built-in function of our evaluation model offers the built-in function called construct_data_message where it will based on the text, label and prediction create the base prompt for all samples in the testset. This will be better than manually replacing the value of each variable. After achieving the testset, we can call some API from OpenAI or Gemini with a for loop to let it generate the data.

```python
evaluateModel = EvaluateModel(
    dataset=testset,
    model_type="ai_detection",
    device="automap"   # feedback_generation/ai_detection
)

evaluateModel.construct_data_message()  # Create the prompt for the dataset

with open(output_file, "w", encoding="utf-8") as f:
    for prompt in evaluateModel.dataset_prompt:
        result = generated_response_gemini(prompt)
```