# Human Evaluation Rubric: AI Detection Task

## Project: AAIE – Artificial Assessment Intelligence for Education
**Task**: Human Evaluation Criteria for AI Detection
**Version**: 1.0
**Date**: 2025

## Overview
This document provides comprehensive evaluation criteria for human raters to assess AI detection model performance. The evaluation uses a binary classification system (Correct Prediction vs. Wrong Prediction) to determine whether the model accurately identified AI-generated vs. human-written text.

## Evaluation Criteria

### Binary Classification System

#### 1. CORRECT PREDICTION 
**Definition**: The model correctly identified the true nature of the text (AI-generated or human-written).

**Criteria for Correct AI Detection**:
- Model predicts "AI" for text that was actually generated by AI
- Model predicts "Human" for text that was actually written by a human

**Examples of Correct Predictions**:
- AI-generated text with repetitive patterns → Model predicts "AI" 
- Human-written text with personal anecdotes → Model predicts "Human" 
- AI-generated text with overly formal language → Model predicts "AI" 
- Human-written text with natural flow and imperfections → Model predicts "Human" 

#### 2. WRONG PREDICTION 
**Definition**: The model incorrectly identified the true nature of the text.

**Criteria for Wrong AI Detection**:
- Model predicts "AI" for text that was actually written by a human
- Model predicts "Human" for text that was actually generated by AI

**Examples of Wrong Predictions**:
- Human-written text with clear structure → Model predicts "AI" 
- AI-generated text with natural variations → Model predicts "Human" 
- Human-written text with technical terminology → Model predicts "AI" 
- AI-generated text with personal details → Model predicts "Human" 

## Detailed Guidelines for Human Raters

### Text Analysis Framework

#### Linguistic Patterns to Consider

**AI-Generated Text Indicators**:
- Unnatural repetition of phrases or sentence structures
- Overly consistent sentence length and complexity
- Lack of personal pronouns or first-person experiences
- Generic or template-like responses
- Perfect grammar and punctuation (too polished)
- Absence of colloquialisms or informal language
- Repetitive use of transition words
- Overly formal or academic tone in casual contexts

**Human-Written Text Indicators**:
- Natural flow and rhythm in writing
- Personal anecdotes or experiences
- Imperfect grammar or punctuation
- Use of colloquialisms and informal language
- Varied sentence structures and lengths
- Emotional or subjective language
- Context-specific references
- Natural transitions between ideas

#### Context Considerations

**Educational Context**:
- Consider the academic level and subject matter
- Evaluate whether the writing style matches expected student output
- Account for different writing abilities and styles
- Consider cultural and linguistic diversity

**Task-Specific Context**:
- Assess whether the response directly addresses the prompt
- Consider the complexity and depth of the response
- Evaluate appropriateness for the given task or question

### Rating Process

#### Step 1: Initial Reading
- Read the text completely without making immediate judgments
- Note the overall tone, style, and content
- Identify any obvious patterns or characteristics

#### Step 2: Pattern Analysis
- Look for linguistic patterns that suggest AI or human origin
- Consider the context and purpose of the text
- Evaluate consistency with expected human writing patterns

#### Step 3: Decision Making
- Determine the true nature of the text (AI-generated or human-written)
- Compare with the model's prediction
- Classify as Correct or Wrong Prediction

#### Step 4: Confidence Assessment
- Rate your confidence in the classification (High/Medium/Low)
- Note any ambiguous cases for team discussion

### Quality Assurance Guidelines

#### Consistency Checks
- **Inter-rater Reliability**: Compare ratings with other team members
- **Intra-rater Consistency**: Re-evaluate samples after time intervals
- **Blind Evaluation**: Avoid knowing the model's prediction initially

#### Dispute Resolution
- **High Disagreement Cases**: Discuss with team members
- **Ambiguous Examples**: Flag for team review and consensus
- **Edge Cases**: Document for future rubric refinement

#### Documentation Requirements
- Record the reasoning for each classification
- Note any patterns or observations
- Document difficult or ambiguous cases

## Evaluation Examples

### Example 1: AI-Generated Text
**Text**: "Regular exercise provides numerous health benefits including improved cardiovascular function, enhanced metabolic efficiency, and optimized cognitive performance. Furthermore, physical activity contributes to stress reduction and emotional well-being."

**Analysis**:
- Overly formal language for casual context
- Perfect sentence structure and grammar
- Generic benefits without personal experience
- Template-like response pattern

**True Classification**: AI-Generated
**Model Prediction**: AI
**Evaluation Result**: CORRECT PREDICTION

### Example 2: Human-Written Text
**Text**: "I started going to the gym last month and it's been really tough but I'm starting to feel better. My friend Sarah goes with me sometimes which helps a lot. I used to be so tired all the time but now I have more energy."

**Analysis**:
- Personal pronouns and experiences
- Natural, conversational tone
- Specific details (friend's name, timeline)
- Imperfect grammar ("it's been really tough but")
- Emotional language ("really tough", "helps a lot")

**True Classification**: Human-Written
**Model Prediction**: Human
**Evaluation Result**: CORRECT PREDICTION

### Example 3: Wrong Prediction Case
**Text**: "The benefits of exercise include better health, more energy, and feeling good. Exercise is important for everyone to do regularly."

**Analysis**:
- Simple, straightforward language
- Natural sentence flow
- Appropriate for educational context
- Could be written by either AI or human

**True Classification**: Human-Written
**Model Prediction**: AI
**Evaluation Result**: WRONG PREDICTION

## Reporting and Documentation

### Evaluation Summary Template
```
Text ID: [ID]
True Classification: [AI/Human]
Model Prediction: [AI/Human]
Evaluation Result: [Correct/Wrong]
Confidence Level: [High/Medium/Low]
Key Observations: [Brief notes on patterns]
Rater: [Name]
Date: [Date]
```

### Metrics to Track
- **Accuracy Rate**: Percentage of correct predictions
- **False Positive Rate**: AI predictions on human text
- **False Negative Rate**: Human predictions on AI text
- **Inter-rater Agreement**: Consistency across evaluators

## Training and Calibration

### New Rater Onboarding
1. **Theory Training**: Understanding of AI vs. human text characteristics
2. **Practice Sessions**: Evaluate sample texts with known classifications
3. **Feedback and Discussion**: Review results with experienced raters
4. **Calibration**: Ensure alignment with established standards

### Ongoing Quality Assurance
- **Regular Review Sessions**: Discuss difficult cases and edge examples
- **Rubric Refinement**: Update criteria based on new patterns
- **Performance Monitoring**: Track individual and team consistency
- **Continuous Learning**: Stay updated on AI text generation capabilities

## Conclusion

This rubric provides a structured approach for human evaluators to consistently assess AI detection model performance. By following these guidelines, team members can ensure reliable and comparable evaluations that contribute to meaningful model assessment and improvement.

**Note**: This rubric should be reviewed and updated regularly as AI text generation capabilities evolve and new patterns emerge.
