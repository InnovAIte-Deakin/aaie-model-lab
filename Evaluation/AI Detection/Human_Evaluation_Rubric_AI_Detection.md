# Human Evaluation Rubric: AI Detection Task

## Project: AAIE – Artificial Assessment Intelligence for Education
**Task**: Human Evaluation Criteria for AI Detection

## Overview
This document provides comprehensive evaluation criteria for human raters to assess AI detection model performance. The evaluation uses a three-class classification system (AI, Human, Hybrid) to determine whether the model accurately identified the true nature of the text.

## Evaluation Criteria

### Three-Class Classification System

#### 1. CORRECT PREDICTION 
**Definition**: The model correctly identified the true nature of the text.

**Criteria for Correct AI Detection**:
- Model predicts "AI" for text that was actually generated by AI
- Model predicts "Human" for text that was actually written by a human
- Model predicts "Hybrid" for text that was created through human-AI collaboration

**Examples of Correct Predictions**:
- AI-generated text with repetitive patterns → Model predicts "AI" 
- Human-written text with personal anecdotes → Model predicts "Human" 
- Text with AI assistance but human editing → Model predicts "Hybrid"
- AI-generated text with overly formal language → Model predicts "AI" 
- Human-written text with natural flow and imperfections → Model predicts "Human" 
- Human-written text with AI-generated sections → Model predicts "Hybrid"

#### 2. WRONG PREDICTION 
**Definition**: The model incorrectly identified the true nature of the text.

**Criteria for Wrong AI Detection**:
- Model predicts "AI" for text that was actually written by a human
- Model predicts "Human" for text that was actually generated by AI
- Model predicts "Hybrid" for text that was purely AI or purely human
- Model predicts "AI" or "Human" for text that was actually hybrid

**Examples of Wrong Predictions**:
- Human-written text with clear structure → Model predicts "AI" 
- AI-generated text with natural variations → Model predicts "Human" 
- Human-written text with technical terminology → Model predicts "AI" 
- AI-generated text with personal details → Model predicts "Human" 
- Pure AI text → Model predicts "Hybrid"
- Pure human text → Model predicts "Hybrid"

## Detailed Guidelines for Human Raters

### Text Analysis Framework

#### Linguistic Patterns to Consider

**AI-Generated Text Indicators**:
- Unnatural repetition of phrases or sentence structures
- Overly consistent sentence length and complexity
- Lack of personal pronouns or first-person experiences
- Generic or template-like responses
- Perfect grammar and punctuation (too polished)
- Absence of colloquialisms or informal language
- Repetitive use of transition words
- Overly formal or academic tone in casual contexts
- Lack of personal voice or unique perspective
- Overly comprehensive coverage without depth

**Human-Written Text Indicators**:
- Natural flow and rhythm in writing
- Personal anecdotes or experiences
- Imperfect grammar or punctuation
- Use of colloquialisms and informal language
- Varied sentence structures and lengths
- Emotional or subjective language
- Context-specific references
- Natural transitions between ideas
- Personal voice and unique perspective
- Inconsistent depth of coverage

**Hybrid Text Indicators**:
- Mix of formal and informal language patterns
- Some sections with perfect grammar, others with natural imperfections
- Combination of generic and personal content
- AI-generated structure with human personalization
- Human-written content with AI-enhanced vocabulary
- Inconsistent writing style throughout the text
- Some sections lack personal voice, others are highly personal
- Mix of template-like and original content
- AI-generated facts with human opinions or experiences
- Professional tone with personal touches

#### Context Considerations

**Educational Context**:
- Consider the academic level and subject matter
- Evaluate whether the writing style matches expected student output
- Account for different writing abilities and styles
- Consider cultural and linguistic diversity
- Assess if AI tools are commonly used in the educational setting

**Task-Specific Context**:
- Assess whether the response directly addresses the prompt
- Consider the complexity and depth of the response
- Evaluate appropriateness for the given task or question
- Consider if the task typically involves AI assistance

### Rating Process

#### Step 1: Initial Reading
- Read the text completely without making immediate judgments
- Note the overall tone, style, and content
- Identify any obvious patterns or characteristics
- Look for signs of mixed authorship

#### Step 2: Pattern Analysis
- Look for linguistic patterns that suggest AI, human, or hybrid origin
- Consider the context and purpose of the text
- Evaluate consistency with expected human writing patterns
- Identify sections that may have different authorship

#### Step 3: Decision Making
- Determine the true nature of the text (AI-generated, human-written, or hybrid)
- Compare with the model's prediction
- Classify as Correct or Wrong Prediction
- Consider the dominant authorship if hybrid

#### Step 4: Confidence Assessment
- Rate your confidence in the classification (High/Medium/Low)
- Note any ambiguous cases for team discussion
- Document reasoning for hybrid classifications

### Quality Assurance Guidelines

#### Consistency Checks
- **Inter-rater Reliability**: Compare ratings with other team members
- **Intra-rater Consistency**: Re-evaluate samples after time intervals
- **Blind Evaluation**: Avoid knowing the model's prediction initially

#### Dispute Resolution
- **High Disagreement Cases**: Discuss with team members
- **Ambiguous Examples**: Flag for team review and consensus
- **Edge Cases**: Document for future rubric refinement
- **Hybrid Classifications**: Ensure team agreement on criteria

#### Documentation Requirements
- Record the reasoning for each classification
- Note any patterns or observations
- Document difficult or ambiguous cases
- Specify which sections appear AI vs. human in hybrid texts

## Evaluation Examples

### Example 1: AI-Generated Text
**Text**: "Regular exercise provides numerous health benefits including improved cardiovascular function, enhanced metabolic efficiency, and optimized cognitive performance. Furthermore, physical activity contributes to stress reduction and emotional well-being."

**Analysis**:
- Overly formal language for casual context
- Perfect sentence structure and grammar
- Generic benefits without personal experience
- Template-like response pattern
- No personal voice or unique perspective

**True Classification**: AI-Generated
**Model Prediction**: AI
**Evaluation Result**: CORRECT PREDICTION

### Example 2: Human-Written Text
**Text**: "I started going to the gym last month and it's been really tough but I'm starting to feel better. My friend Sarah goes with me sometimes which helps a lot. I used to be so tired all the time but now I have more energy."

**Analysis**:
- Personal pronouns and experiences
- Natural, conversational tone
- Specific details (friend's name, timeline)
- Imperfect grammar ("it's been really tough but")
- Emotional language ("really tough", "helps a lot")
- Strong personal voice

**True Classification**: Human-Written
**Model Prediction**: Human
**Evaluation Result**: CORRECT PREDICTION

### Example 3: Hybrid Text
**Text**: "Exercise provides numerous health benefits including improved cardiovascular function and enhanced metabolic efficiency. I personally started going to the gym last month and it's been challenging but rewarding. My friend Sarah joins me sometimes, which makes the experience more enjoyable. Regular physical activity contributes to stress reduction and emotional well-being, and I can definitely attest to feeling more energetic throughout the day."

**Analysis**:
- Mix of formal, AI-like language and personal experiences
- Some sections with perfect grammar, others with natural flow
- Generic health benefits combined with personal anecdotes
- Professional tone with personal touches
- AI-generated structure with human personalization

**True Classification**: Hybrid
**Model Prediction**: Hybrid
**Evaluation Result**: CORRECT PREDICTION

### Example 4: Wrong Prediction Case
**Text**: "The benefits of exercise include better health, more energy, and feeling good. Exercise is important for everyone to do regularly."

**Analysis**:
- Simple, straightforward language
- Natural sentence flow
- Appropriate for educational context
- Could be written by either AI or human

**True Classification**: Human-Written
**Model Prediction**: AI
**Evaluation Result**: WRONG PREDICTION

## Reporting and Documentation

### Evaluation Summary Template
```
Text ID: [ID]
True Classification: [AI/Human/Hybrid]
Model Prediction: [AI/Human/Hybrid]
Evaluation Result: [Correct/Wrong]
Confidence Level: [High/Medium/Low]
Key Observations: [Brief notes on patterns]
Hybrid Details: [If hybrid, specify AI vs. human sections]
Rater: [Name]
Date: [Date]
```

### Metrics to Track
- **Accuracy Rate**: Percentage of correct predictions
- **False Positive Rate**: AI predictions on human text
- **False Negative Rate**: Human predictions on AI text
- **Hybrid Misclassification Rate**: Incorrect hybrid classifications
- **Inter-rater Agreement**: Consistency across evaluators

## Training and Calibration

### New Rater Onboarding
1. **Theory Training**: Understanding of AI, human, and hybrid text characteristics
2. **Practice Sessions**: Evaluate sample texts with known classifications
3. **Feedback and Discussion**: Review results with experienced raters
4. **Calibration**: Ensure alignment with established standards
5. **Hybrid Training**: Special focus on identifying mixed authorship

### Ongoing Quality Assurance
- **Regular Review Sessions**: Discuss difficult cases and edge examples
- **Rubric Refinement**: Update criteria based on new patterns
- **Performance Monitoring**: Track individual and team consistency
- **Continuous Learning**: Stay updated on AI text generation capabilities
- **Hybrid Case Studies**: Regular review of hybrid text examples

## Conclusion

This rubric provides a structured approach for human evaluators to consistently assess AI detection model performance across three classification categories. By following these guidelines, team members can ensure reliable and comparable evaluations that contribute to meaningful model assessment and improvement, including the increasingly common hybrid human-AI collaborative texts.