# Evaluation Framework - AAIE Project

## Project: AAIE – Artificial Assessment Intelligence for Education
**Task**: Human Evaluation Criteria Development
**Version**: 1.0
**Date**: 2025

## Overview

This directory contains comprehensive human evaluation rubrics for assessing AI model performance in two critical tasks:

1. **AI Detection Task**: Binary classification evaluation (Correct vs. Wrong Prediction)
2. **Feedback Generation Task**: 1-5 scale evaluation with detailed criteria

## Directory Structure

```
Evaluation/
├── README.md                           # This overview document
├── AI Detection/
│   ├── Human_Evaluation_Rubric_AI_Detection.md
│   └── AI_Detection_Evaluation_Rubric_Detailed.docx
├── Feedback Generation/
│   ├── Human_Evaluation_Rubric_Feedback_Generation.md
│   └── Feedback_Generation_Evaluation_Rubric_Detailed.docx
└── dummy.csv                          # Placeholder file
```

## Documentation Formats

This evaluation framework provides documentation in two complementary formats:

### **Markdown Files (.md)**
- **Purpose**: Quick reference and daily use
- **Content**: Essential criteria, examples, and guidelines
- **Use Case**: Regular evaluation work and quick lookups
- **Format**: Lightweight, easy to read in any text editor

### **Word Documents (.docx)**
- **Purpose**: Comprehensive training and detailed reference
- **Content**: Expanded explanations, detailed examples, training materials
- **Use Case**: New evaluator training, complex cases, detailed analysis
- **Format**: Rich text with formatting, suitable for printing and sharing

## Quick Start Guide

### For New Team Members

1. **Read this README** to understand the evaluation framework
2. **Review the specific rubric** for your assigned task:
   - AI Detection: Use binary classification (Correct/Wrong)
   - Feedback Generation: Use 1-5 rating scale
3. **Complete training** with sample evaluations
4. **Begin evaluation** following the established guidelines

### For Evaluators

1. **Choose your task** (AI Detection or Feedback Generation)
2. **Follow the step-by-step process** outlined in each rubric
3. **Use the provided templates** for consistent documentation
4. **Participate in calibration sessions** to maintain quality

## Evaluation Tasks

### 1. AI Detection Task

**Objective**: Evaluate whether the AI model correctly identifies AI-generated vs. human-written text

**Evaluation Method**: Binary Classification
- ✅ **Correct Prediction**: Model accurately identifies text origin
- ❌ **Wrong Prediction**: Model incorrectly identifies text origin

**Key Criteria**:
- Linguistic patterns analysis
- Context considerations
- Consistency across evaluators
- Quality assurance measures

**Use Case**: When you need to assess if the AI can distinguish between human and AI-generated content

**Detailed Documentation**: 
- **Quick Reference**: `Human_Evaluation_Rubric_AI_Detection.md` (Markdown format)
- **Comprehensive Guide**: `AI_Detection_Evaluation_Rubric_Detailed.docx` (Word format with expanded explanations)

### 2. Feedback Generation Task

**Objective**: Evaluate the quality of AI-generated feedback for students

**Evaluation Method**: 1-5 Rating Scale
- **5 (Excellent)**: Outstanding feedback exceeding expectations
- **4 (Good)**: High-quality feedback meeting most expectations
- **3 (Average)**: Adequate feedback meeting basic requirements
- **2 (Poor)**: Below-average feedback with significant issues
- **1 (Unacceptable)**: Inadequate feedback failing requirements

**Key Criteria**:
- **Correctness**: Accuracy and helpfulness
- **Clarity**: Understandability and communication
- **Tone**: Supportiveness and constructiveness
- **Actionability**: Clear next steps and implementation

**Use Case**: When you need to assess the quality of AI-generated educational feedback

**Detailed Documentation**: 
- **Quick Reference**: `Human_Evaluation_Rubric_Feedback_Generation.md` (Markdown format)
- **Comprehensive Guide**: `Feedback_Generation_Evaluation_Rubric_Detailed.docx` (Word format with expanded explanations)

## Quality Assurance

### Inter-Rater Reliability
- Regular calibration sessions
- Sample review meetings
- Dispute resolution processes
- Consistency monitoring

### Training Requirements
- New rater onboarding
- Practice sessions with samples
- Feedback on rating quality
- Continuous improvement processes

### Documentation Standards
- Rating justification
- Evidence examples
- Context notes
- Improvement suggestions

## Best Practices

### Before Evaluation
1. **Understand the context** of the evaluation task
2. **Review the rubric** thoroughly
3. **Practice with sample materials** if available
4. **Ensure you're in the right mindset** for objective evaluation

### During Evaluation
1. **Follow the step-by-step process** outlined in the rubric
2. **Take notes** on your reasoning
3. **Be consistent** with your application of criteria
4. **Flag any ambiguous cases** for team discussion

### After Evaluation
1. **Document your ratings** using the provided templates
2. **Review your work** for consistency
3. **Participate in team discussions** about difficult cases
4. **Provide feedback** on the rubric and process

## Common Challenges and Solutions

### Challenge: Inconsistent Ratings Across Evaluators
**Solution**: Participate in calibration sessions and use the detailed criteria consistently

### Challenge: Ambiguous or Edge Cases
**Solution**: Document these cases and discuss with the team for consensus

### Challenge: Maintaining Objectivity
**Solution**: Follow the structured evaluation process and focus on evidence-based assessment

### Challenge: Understanding Complex Criteria
**Solution**: Review examples in the rubric and ask for clarification from experienced team members

## Reporting and Metrics

### Individual Evaluation Template
Each evaluation should include:
- Task identification
- Rating(s) with justification
- Evidence examples
- Confidence level
- Any notes or observations

### Team Metrics
Track the following across all evaluations:
- Inter-rater agreement rates
- Rating distribution patterns
- Common areas of disagreement
- Quality improvement trends

## Getting Help

### Questions About the Rubric
- Review the detailed criteria in each rubric document
- Check the examples provided
- Ask experienced team members

### Technical Issues
- Contact the project lead
- Document the issue for future rubric updates

### Quality Concerns
- Participate in team calibration sessions
- Request additional training if needed
- Provide feedback on rubric clarity

## Future Updates

This evaluation framework is designed to evolve based on:
- Team feedback and usage experience
- Changes in AI capabilities
- Educational requirements updates
- Research findings and best practices

**Note**: Always use the most current version of the rubrics and participate in feedback sessions to help improve the framework.

---

## Contact Information

For questions about this evaluation framework, please contact the project team or refer to the project documentation.

**Last Updated**: 2025
**Version**: 1.0
