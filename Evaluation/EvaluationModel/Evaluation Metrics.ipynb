{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c12340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T08:15:07.874586Z",
     "iopub.status.busy": "2025-08-09T08:15:07.873580Z",
     "iopub.status.idle": "2025-08-09T08:15:18.747019Z",
     "shell.execute_reply": "2025-08-09T08:15:18.745838Z"
    },
    "id": "9CLy4LADSXo3",
    "papermill": {
     "duration": 10.882001,
     "end_time": "2025-08-09T08:15:18.748353",
     "exception": true,
     "start_time": "2025-08-09T08:15:07.866352",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/3122566948.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleu_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSmoothingFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrouge_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrouge_scorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbert_score_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rouge_score'"
     ]
    }
   ],
   "source": [
    "#!pip install rouge_score bert_score\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from bert_score import score as bert_score_fn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,  confusion_matrix, classification_report)\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, Trainer, TrainingArguments\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5850f7",
   "metadata": {
    "id": "jBBe5gsLKDnp",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#1. EvaluateModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f8c79e",
   "metadata": {
    "id": "vNnRMeWujflf",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## i. Dataset Structure Instruction\n",
    "### a. Classification Model Evaluation\n",
    "dataset = {\n",
    "\n",
    "    \"prompts\": [\"\", \"\", \"\", ...], #List of the text,\n",
    "    \"labels\": [\"AI\", \"Human\", \"neutral\", ...],  # Ground-truth labels (can be int or str)\n",
    "    \"predictions\": [\"positive\", \"positive\", \"neutral\", ...]  # Predicted labels (same format as `label`)\n",
    "}\n",
    "\n",
    "We can evaluate the model as\n",
    "```\n",
    "evaluator = EvaluateModel(dataset=dataset)\n",
    "evaluator.evaluate_classification_model(average='macro',print_result)\n",
    "```\n",
    "The function already support label encoding automatically if your labels are strings.\n",
    "\n",
    "\n",
    "### b. Generative Model Evaluation\n",
    "\n",
    "```\n",
    "dataset = {\n",
    "    \"reference_texts\": [\n",
    "        [\"The cat sat on the mat.\", \"A cat is sitting on a mat.\"],\n",
    "        [\"Hello, how are you?\"]\n",
    "        # Each item is a list of reference texts\n",
    "    ],\n",
    "    \"generated_texts\": [\n",
    "        \"The cat is sitting on the mat.\",\n",
    "        \"Hi, how do you do?\"\n",
    "        # Each item is a generated hypothesis\n",
    "    ]\n",
    "}\n",
    "```\n",
    "We can evaluate the model as\n",
    "\n",
    "\n",
    "```\n",
    "evaluator = EvaluateModel(dataset=dataset)\n",
    "evaluator.evaluate_generative_model(metrics=['bleu', 'rouge', 'bertscore'], print_result=True) # Head to the functio to see more detail\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa47afd",
   "metadata": {
    "id": "rWdj_tT6XWCT",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EvaluateModel():\n",
    "  def __init__(self, dataset=None, model=None, tokenizer=None) -> None:\n",
    "      self.model = model\n",
    "      self.tokenizer = tokenizer\n",
    "      self.results = {}\n",
    "      self.device = ( \"cuda\" if torch.cuda.is_available()\n",
    "                    else \"mps\" if torch.backends.mps.is_available()\n",
    "                    else \"cpu\")\n",
    "\n",
    "      self.dataset = dataset\n",
    "\n",
    "  #Check the labels is int and encode it if it is string\n",
    "  def encode_labels(self, col):\n",
    "      try:\n",
    "          int_labels = [int(x) for x in self.dataset[col]]\n",
    "          label_names = sorted(set(int_labels))\n",
    "      except ValueError:\n",
    "          le = LabelEncoder()\n",
    "          encoded_labels = le.fit_transform(self.dataset[col])\n",
    "          int_labels = encoded_labels\n",
    "          label_names = sorted(set(encoded_labels))  # integer labels only\n",
    "\n",
    "          print(f\"The {col} labels have been encoded. Integer mapping:\")\n",
    "          for original, encoded in zip(le.classes_, le.transform(le.classes_)):\n",
    "              print(f\"{original} -> {encoded}\")\n",
    "\n",
    "      return int_labels, label_names\n",
    "\n",
    "  #Compute the BLEU score\n",
    "  def bleu_score(self, reference_texts, generated_text):\n",
    "      ref_tokens = [ref.split() for ref in reference_texts]\n",
    "      gen_tokens = generated_text.split()\n",
    "      smoothie = SmoothingFunction().method1\n",
    "      bleu = sentence_bleu(ref_tokens, gen_tokens, smoothing_function=smoothie)\n",
    "      return {'bleu': bleu}\n",
    "\n",
    "  #Compute the ROUGE score\n",
    "  def rouge_score(self, reference_texts, generated_text):\n",
    "      scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "      scores = [scorer.score(ref, generated_text) for ref in reference_texts]\n",
    "      avg_scores = {\n",
    "          k: np.mean([s[k].fmeasure for s in scores]) for k in scores[0]\n",
    "      }\n",
    "      return avg_scores\n",
    "\n",
    "  #Compute the BERTScore\n",
    "  def bert_score(self, reference_texts, generated_text, model_type_ = 'bert-base-uncased'):\n",
    "      P, R, F1 = bert_score_fn([generated_text], [reference_texts], model_type=model_type_)\n",
    "      return {\n",
    "          'bertscore_precision': P.mean().item(),\n",
    "          'bertscore_recall': R.mean().item(),\n",
    "          'bertscore_f1': F1.mean().item()\n",
    "      }\n",
    "\n",
    "  #Evaluate the clasification model\n",
    "  def evaluate_classification_model(self, average='macro', print_result=True):\n",
    "      \"\"\"\n",
    "      Evaluate a classification model using standard metrics:\n",
    "      - Accuracy, Precision, Recall, F1-score, Confusion Matrix\n",
    "\n",
    "      Args:\n",
    "          average (str): Averaging method for multi-class ('binary', 'micro', 'macro').\n",
    "          print_result (bool): Whether to print results and plot confusion matrix.\n",
    "\n",
    "      Workflow:\n",
    "          - Checks for 'label' and 'prediction' keys in dataset.\n",
    "          - Encodes non-integer labels if necessary.\n",
    "          - Computes classification metrics.\n",
    "          - Optionally prints a classification report and confusion matrix.\n",
    "      \"\"\"\n",
    "      if any(k not in self.dataset for k in [\"labels\", \"predictions\"]):\n",
    "          print(\"Please provide the dataset with 'label' and 'prediction' columns.\")\n",
    "          return\n",
    "\n",
    "      # Encode labels (handles string → int mapping if needed)\n",
    "      targets, label_names_target = self.encode_labels(\"labels\")\n",
    "      predictions, label_names_prediction = self.encode_labels(\"predictions\")\n",
    "\n",
    "      label_names = label_names_target\n",
    "      print(targets)\n",
    "      print(predictions)\n",
    "      print(label_names)\n",
    "      # Compute evaluation metrics\n",
    "      acc = accuracy_score(targets, predictions)\n",
    "      prec = precision_score(targets, predictions, average=average, zero_division=0)\n",
    "      rec = recall_score(targets, predictions, average=average, zero_division=0)\n",
    "      f1 = f1_score(targets, predictions, average=average, zero_division=0)\n",
    "      cm = confusion_matrix(targets, predictions, labels=label_names)\n",
    "\n",
    "      # Optionally print results\n",
    "      if print_result:\n",
    "          plt.figure(figsize=(6, 5))\n",
    "          sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "          plt.xlabel('Predicted')\n",
    "          plt.ylabel('Actual')\n",
    "          plt.title('Confusion Matrix')\n",
    "          plt.show()\n",
    "\n",
    "          print(f\"Accuracy: {acc:.4f}\")\n",
    "          print(f\"F1-score: {f1:.4f}\")\n",
    "          print(f\"Recall: {rec:.4f}\")\n",
    "          print(f\"Precision: {prec:.4f}\")\n",
    "          print(\"\\nClassification Report:\")\n",
    "          print(classification_report(targets, predictions, labels=label_names, zero_division=0))\n",
    "\n",
    "      # Store results\n",
    "      self.results.update({\n",
    "          \"accuracy\": acc,\n",
    "          \"precision\": prec,\n",
    "          \"recall\": rec,\n",
    "          \"f1_score\": f1,\n",
    "          \"confusion_matrix\": cm,\n",
    "      })\n",
    "\n",
    "\n",
    "  def evaluate_generative_model(self, metrics=['bleu', 'rouge', 'bertscore'], print_result = True, bert_model='bert-base-uncased'):\n",
    "      \"\"\"\n",
    "      Evaluate a generative model using metrics like BLEU, ROUGE, and BERTScore.\n",
    "\n",
    "      Args:\n",
    "          metrics (list): List of metrics to compute. Options: 'bleu', 'rouge', 'bertscore'.\n",
    "          bert_model (str): Hugging Face model name to use for BERTScore.\n",
    "\n",
    "      Workflow:\n",
    "          - Validates that 'reference_texts' and 'generated_texts' are in the dataset.\n",
    "          - Loops over each pair of reference and generated text.\n",
    "          - Computes and stores selected metrics.\n",
    "      \"\"\"\n",
    "      if \"reference_texts\" not in self.dataset or \"generated_texts\" not in self.dataset:\n",
    "          print(\"Please provide 'reference_texts' and 'generated_texts' in the dataset.\")\n",
    "          return\n",
    "\n",
    "      refs = self.dataset['reference_texts'].copy()\n",
    "      gens = self.dataset['generated_texts'].copy()\n",
    "\n",
    "      for ref, gen in zip(refs, gens):\n",
    "          # Expect `ref` to be a list of strings, `gen` to be a single string\n",
    "          if 'bleu' in metrics:\n",
    "              self.results.update(self.bleu_score(ref, gen))\n",
    "          if 'rouge' in metrics:\n",
    "              self.results.update(self.rouge_score(ref, gen))\n",
    "          if 'bertscore' in metrics:\n",
    "              self.results.update(self.bert_score(ref, gen, bert_model))\n",
    "\n",
    "      if print_result:\n",
    "          print(\"Generative Model Evaluation Results:\")\n",
    "          for metric, score in self.results.items():\n",
    "              print(f\"{metric:20}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9b565",
   "metadata": {
    "id": "qjpPTreaZJN-",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 2. Testing stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f881e73",
   "metadata": {
    "id": "D-7XtdngZL2N",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## a. Testing generateive metrics without loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc956a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xfGKazZY8gM",
    "outputId": "0a257898-3da9-4c67-a66c-de2dd898c2de",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference = \"The quick brown fox jumps over the lazy dog\"\n",
    "generated = \"The fast brown fox jumps over a lazy dog\"\n",
    "\n",
    "evaluateModel = EvaluateModel()\n",
    "\n",
    "bert = evaluateModel.bert_score(\n",
    "    reference_texts=reference,\n",
    "    generated_text=generated,\n",
    ")\n",
    "\n",
    "bleu = evaluateModel.bleu_score(\n",
    "    reference_texts=reference,\n",
    "    generated_text=generated\n",
    ")\n",
    "\n",
    "rouge = evaluateModel.rouge_score(\n",
    "    reference_texts=reference,\n",
    "    generated_text=generated\n",
    ")\n",
    "print(bleu)\n",
    "print(rouge)\n",
    "print(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff117d",
   "metadata": {
    "id": "8oDMNNQVZ0R0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## b. Testing the classification metrics with full evaluation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb10aca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "id": "EZoKTcdEaT9K",
    "outputId": "53860301-04a8-45ab-bd89-80e88d428fff",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"prompt\": [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Neural networks can approximate any function.\",\n",
    "        \"I had coffee this morning and read the news.\"\n",
    "    ],\n",
    "    \"label\": [\n",
    "        \"Human\",\n",
    "        \"AI\",\n",
    "        \"Human\"\n",
    "    ],\n",
    "    \"prediction\": [\n",
    "        \"AI\",\n",
    "        \"AI\",\n",
    "        \"Human\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "evaluateModel = EvaluateModel(dataset=dataset)\n",
    "evaluateModel.evaluate_classification_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbb1333",
   "metadata": {
    "id": "dzOExgTHcvDd",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## c. Testing the Generative model with full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de01ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8i0QUd0XKdE",
    "outputId": "1f456482-5c9d-42ab-9294-d0c9574e39c9",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"reference_texts\": [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Neural networks can approximate any function.\",\n",
    "        \"I had coffee this morning and read the news.\"\n",
    "    ],\n",
    "    \"generated_texts\": [\n",
    "        \"The quick red fox jumps over the lazy dog.\",\n",
    "        \"Neural networks could approximate any function.\",\n",
    "        \"I have coffee this morning and read the news.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "evaluateModel = EvaluateModel(dataset=dataset)\n",
    "evaluateModel.evaluate_generative_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5d468",
   "metadata": {
    "id": "9VLSZkJreMzg",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 3. Experiment Setup for the AI Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8d14b",
   "metadata": {
    "id": "eZ-G40um1YhZ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_text(model, tokenizer, device, text, few_shot_prompt, max_new_tokens=10):\n",
    "    # Insert the new text into the few-shot prompt\n",
    "    prompt = few_shot_prompt.format(text)\n",
    "\n",
    "    # Tokenize and send to device\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output_ids = model.generate(**input_ids, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    # Extract the label predicted after the last 'Label:' token\n",
    "    # For example, output may be the full prompt + \" AI\" or \" Human\"\n",
    "    label = generated_text.split(\"Label:\")[-1].strip().split()[0]\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd1af1",
   "metadata": {
    "id": "5aAJWu7h2zZ-",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "few_shot_prompt = \"\"\"\n",
    "Decide whether the following text was written by a human or an AI.\n",
    "\n",
    "Text: \"Artificial intelligence is a powerful tool for automating tasks.\"\n",
    "Label: AI\n",
    "\n",
    "Text: \"I walked to the market this morning and bought fresh bread.\"\n",
    "Label: Human\n",
    "\n",
    "Text: \"The moon is a celestial body that orbits Earth.\"\n",
    "Label: AI\n",
    "\n",
    "Text: \"Yesterday, I enjoyed a long walk in the park.\"\n",
    "Label: Human\n",
    "\n",
    "Text: \"Machine learning models improve with more data.\"\n",
    "Label: AI\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1cd633",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257,
     "referenced_widgets": [
      "9397036efb8345aa9f535a83b85d8ae4",
      "2231a05ab9fc4444a42f745b54f5f7f7",
      "979596b2d565450b8f1c013b6849cbc0",
      "56754b314fd34af6839813cc232284a5",
      "a6ef66cf4f3c4da4b3e30e195c9f8b07",
      "332f10d19ac4410690ad34e6ebe2c051",
      "78f0201fdd234e19b64aa7177ea49370",
      "1e639d894a174f5f9dd98181066e108c",
      "9d142956e93d46509a4e394964da42c1",
      "0e48d5dbab8246b6a00c9997de8dd701",
      "85b3c72dd41c472e9441f97a6aec5287"
     ]
    },
    "id": "cRiblVh787bV",
    "outputId": "84ec4079-f779-4b95-800a-cba9e6b720f3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"prompt\": [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Neural networks can approximate any function.\",\n",
    "        \"I had coffee this morning and read the news.\"\n",
    "    ],\n",
    "    \"labels\": [\n",
    "        \"Human\",\n",
    "        \"AI\",\n",
    "        \"Human\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "#Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "pred_label_ls = []\n",
    "\n",
    "#Run the model and get the label\n",
    "for i, text in enumerate(dataset[\"prompt\"]):\n",
    "    pred_label = classify_text(model, tokenizer, device, text, few_shot_prompt)\n",
    "    pred_label_ls.append(pred_label)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Label: {pred_label} | Actual Label: {dataset['labels'][i]}\")\n",
    "    print()\n",
    "\n",
    "#Store the prediction to dataset\n",
    "dataset['predictions'] = pred_label_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d6ef2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 886
    },
    "id": "iyw8YFg9haVW",
    "outputId": "6e8c2d5a-ac89-4ba6-de7a-26edb967bcb7",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluateModel = EvaluateModel(dataset=dataset)\n",
    "evaluateModel.evaluate_classification_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9e212",
   "metadata": {
    "id": "dk10ip-Pmy5G",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.185985,
   "end_time": "2025-08-09T08:15:21.031657",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-09T08:15:02.845672",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e48d5dbab8246b6a00c9997de8dd701": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e639d894a174f5f9dd98181066e108c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2231a05ab9fc4444a42f745b54f5f7f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_332f10d19ac4410690ad34e6ebe2c051",
      "placeholder": "​",
      "style": "IPY_MODEL_78f0201fdd234e19b64aa7177ea49370",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "332f10d19ac4410690ad34e6ebe2c051": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56754b314fd34af6839813cc232284a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e48d5dbab8246b6a00c9997de8dd701",
      "placeholder": "​",
      "style": "IPY_MODEL_85b3c72dd41c472e9441f97a6aec5287",
      "value": " 2/2 [00:27&lt;00:00, 11.60s/it]"
     }
    },
    "78f0201fdd234e19b64aa7177ea49370": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85b3c72dd41c472e9441f97a6aec5287": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9397036efb8345aa9f535a83b85d8ae4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2231a05ab9fc4444a42f745b54f5f7f7",
       "IPY_MODEL_979596b2d565450b8f1c013b6849cbc0",
       "IPY_MODEL_56754b314fd34af6839813cc232284a5"
      ],
      "layout": "IPY_MODEL_a6ef66cf4f3c4da4b3e30e195c9f8b07"
     }
    },
    "979596b2d565450b8f1c013b6849cbc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e639d894a174f5f9dd98181066e108c",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9d142956e93d46509a4e394964da42c1",
      "value": 2
     }
    },
    "9d142956e93d46509a4e394964da42c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a6ef66cf4f3c4da4b3e30e195c9f8b07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
