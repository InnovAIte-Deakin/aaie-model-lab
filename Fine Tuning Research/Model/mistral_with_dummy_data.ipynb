{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFw4TJp-uhPz",
        "outputId": "0782470c-26fd-499d-ad6d-d3b741f535c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.3.1+cu121\n",
            "CUDA available: True\n",
            "CUDA (torch): 12.1\n",
            "Sun Aug 10 00:13:39 2025 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 580.65.06 Driver Version: 580.65.06 CUDA Version: 13.0 | +-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | [...]\n"
          ]
        }
      ],
      "source": [
        "import torch, subprocess, textwrap\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA (torch):\", torch.version.cuda)\n",
        "print(textwrap.shorten(subprocess.run([\"nvidia-smi\"], text=True, capture_output=True).stdout, width=500))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UctynnHXTWwZ",
        "outputId": "4e493415-b3da-4d91-9fd2-6d130743f10f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Aug 10 00:14:25 2025       \r\n",
            "+-----------------------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 580.65.06              Driver Version: 580.65.06      CUDA Version: 13.0     |\r\n",
            "+-----------------------------------------+------------------------+----------------------+\r\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                                         |                        |               MIG M. |\r\n",
            "|=========================================+========================+======================|\r\n",
            "|   0  Tesla T4                       On  |   00000000:00:04.0 Off |                    0 |\r\n",
            "| N/A   38C    P8             13W /   70W |       3MiB /  15360MiB |      0%      Default |\r\n",
            "|                                         |                        |                  N/A |\r\n",
            "+-----------------------------------------+------------------------+----------------------+\r\n",
            "\r\n",
            "+-----------------------------------------------------------------------------------------+\r\n",
            "| Processes:                                                                              |\r\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\r\n",
            "|        ID   ID                                                               Usage      |\r\n",
            "|=========================================================================================|\r\n",
            "|  No running processes found                                                             |\r\n",
            "+-----------------------------------------------------------------------------------------+\r\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp3zSgDJbb22",
        "outputId": "4d5d4f32-b7bf-4285-dffe-1d0ef7623f0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/namit/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Fetching 3 files: 100%|███████████████████████████| 3/3 [02:52<00:00, 57.64s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/home/namit/mistral_models/7B-Instruct-v0.3'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "from pathlib import Path\n",
        "\n",
        "mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\n",
        "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "snapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KD6WwmLLV0o",
        "outputId": "8cb93a78-91c5-47f7-dd66-aa1f11e58bbf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████████████| 3/3 [01:24<00:00, 28.13s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MistralForCausalLM(\n",
              "  (model): MistralModel(\n",
              "    (embed_tokens): Embedding(32768, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x MistralDecoderLayer(\n",
              "        (self_attn): MistralSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): MistralRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): MistralMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                         bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=bnb\n",
        ")\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IZG2Vp8RnVu",
        "outputId": "6c51f823-9d49-4e14-f2da-fe982a37ed2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In this lab I used ggplot2 to visualize trends and discussed limitations.\n",
            "{'label': 'HUMAN', 'confidence': 0.8, 'rationale': 'Specific software tool mention and personal lab context'} \n",
            "\n",
            "As an AI language model, I will now provide a comprehensive response.\n",
            "{'label': 'AI', 'confidence': 0.99, 'rationale': 'Meta-disclaimer and templated phrasing'} \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json, torch\n",
        "\n",
        "# ---- reply-only helper (unchanged) ----\n",
        "def chat_reply_only(messages, max_new_tokens=140, do_sample=False, temperature=0.0, top_p=1.0):\n",
        "    text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tok([text], return_tensors=\"pt\").to(model.device)\n",
        "    prompt_len = inputs[\"input_ids\"].shape[-1]\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=do_sample,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            pad_token_id=tok.eos_token_id,\n",
        "        )\n",
        "    gen_ids = out[0][prompt_len:]\n",
        "    return tok.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "# ---- strict instruction with 2 examples ----\n",
        "SYSTEM_DETECT = \"\"\"\n",
        "You are an academic integrity assistant. Classify the text as HUMAN or AI GENERATED.\n",
        "Return ONLY a single JSON object (no prose, no markdown, no code fences).\n",
        "Schema: {\"label\":\"HUMAN\"|\"AI\",\"confidence\":0-1,\"rationale\":\"<one sentence>\"}\n",
        "\n",
        "Examples:\n",
        "Text: \"As an AI language model, I cannot browse the internet but I can provide...\"\n",
        "Output: {\"label\":\"AI\",\"confidence\":0.85,\"rationale\":\"Meta-disclaimer and templated phrasing\"}\n",
        "\n",
        "Text: \"I collected results over three trials and wrote a limitation section about sampling bias.\"\n",
        "Output: {\"label\":\"HUMAN\",\"confidence\":0.78,\"rationale\":\"Personal procedure details and idiosyncratic phrasing\"}\n",
        "\n",
        "Now classify the next text.\n",
        "\"\"\".strip()\n",
        "\n",
        "# ---- balanced JSON extractor (no regex recursion needed) ----\n",
        "def extract_first_json(s: str):\n",
        "    # find first '{'\n",
        "    start = s.find('{')\n",
        "    if start == -1:\n",
        "        raise ValueError(\"no opening brace\")\n",
        "    depth = 0\n",
        "    in_str = False\n",
        "    esc = False\n",
        "    for i in range(start, len(s)):\n",
        "        ch = s[i]\n",
        "        if in_str:\n",
        "            if esc:\n",
        "                esc = False\n",
        "            elif ch == '\\\\':\n",
        "                esc = True\n",
        "            elif ch == '\"':\n",
        "                in_str = False\n",
        "        else:\n",
        "            if ch == '\"':\n",
        "                in_str = True\n",
        "            elif ch == '{':\n",
        "                depth += 1\n",
        "            elif ch == '}':\n",
        "                depth -= 1\n",
        "                if depth == 0:\n",
        "                    return json.loads(s[start:i+1])\n",
        "    raise ValueError(\"no balanced json\")\n",
        "\n",
        "\n",
        "def detect_ai(text: str, attempts: int = 3):\n",
        "    for i in range(attempts):\n",
        "        do_sample = (i > 0)   # first try greedy, then allow mild sampling\n",
        "        temp, top_p = ((0.3, 0.9) if do_sample else (0.0, 1.0))\n",
        "        reply = chat_reply_only(\n",
        "            [{\"role\":\"system\",\"content\":SYSTEM_DETECT},\n",
        "             {\"role\":\"user\",\"content\":f\"Text:\\n{text}\\n\\nOutput must be valid JSON only, with double quotes and no trailing commas.\"}],\n",
        "            max_new_tokens=150, do_sample=do_sample, temperature=temp, top_p=top_p\n",
        "        )\n",
        "       \n",
        "        try:\n",
        "            out = extract_first_json(reply)\n",
        "            lab = out.get(\"label\", \"HUMAN\")\n",
        "            if lab not in (\"HUMAN\",\"AI\"):\n",
        "                lab = \"HUMAN\"\n",
        "            conf = float(out.get(\"confidence\", 0.5))\n",
        "            conf = max(0.0, min(1.0, conf))\n",
        "            rat = out.get(\"rationale\", \"\")\n",
        "            return {\"label\": lab, \"confidence\": conf, \"rationale\": rat}\n",
        "        except Exception:\n",
        "            continue\n",
        "        return {\"label\": \"HUMAN\", \"confidence\": 0.5, \"rationale\": \"Parsing failed; review manually.\"}\n",
        "\n",
        "# ---- quick smoke test ----\n",
        "tests = [\n",
        "    \"In this lab I used ggplot2 to visualize trends and discussed limitations.\",\n",
        "    \"As an AI language model, I will now provide a comprehensive response.\"\n",
        "]\n",
        "for t in tests:\n",
        "    print(t)\n",
        "    print(detect_ai(t), \"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMxfwOGCUk9X"
      },
      "source": [
        " Create Dummy Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hBNPRziq8Sx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create dummy HUMAN/AI dataset\n",
        "data = [\n",
        "    {\"text\": \"In this lab I used ggplot2 to visualize trends and discussed limitations.\", \"label\": \"HUMAN\"},\n",
        "    {\"text\": \"As an AI language model, I will now provide a comprehensive response.\", \"label\": \"AI\"},\n",
        "    {\"text\": \"Our company achieved record profits this quarter, exceeding expectations.\", \"label\": \"HUMAN\"},\n",
        "    {\"text\": \"Here is a list of 10 ways to improve productivity:\", \"label\": \"AI\"},\n",
        "    {\"text\": \"The experiment yielded consistent results across multiple trials.\", \"label\": \"HUMAN\"},\n",
        "    {\"text\": \"I am an AI assistant programmed to help you with your queries.\", \"label\": \"AI\"}\n",
        "]\n",
        "df = pd.DataFrame(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt6K9NNwUhre"
      },
      "source": [
        " Evaluate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfk6m1BQFCmQ",
        "outputId": "07b9a6ff-b704-4510-d264-06c5d13a0791"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/namit/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "preds, gts, times = [], [], []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    start = time.time()\n",
        "    out = detect_ai(row[\"text\"])\n",
        "    times.append(time.time() - start)\n",
        "    preds.append(out[\"label\"])\n",
        "    gts.append(row[\"label\"])\n",
        "\n",
        "# Compute metrics\n",
        "acc = accuracy_score(gts, preds)\n",
        "prec = precision_score(gts, preds, pos_label=\"AI\")\n",
        "rec = recall_score(gts, preds, pos_label=\"AI\")\n",
        "f1 = f1_score(gts, preds, pos_label=\"AI\")\n",
        "avg_latency = sum(times) / len(times)\n",
        "\n",
        "perf_report = {\n",
        "    \"n_samples\": len(df),\n",
        "    \"accuracy\": acc,\n",
        "    \"precision\": prec,\n",
        "    \"recall\": rec,\n",
        "    \"f1\": f1,\n",
        "    \"avg_latency_sec\": avg_latency\n",
        "}\n",
        "\n",
        "# Save performance report\n",
        "pd.DataFrame([perf_report]).to_csv(\"performance_report.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koELHqtaUf9Z"
      },
      "source": [
        " Cost Notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqk95xfbUQrR"
      },
      "outputs": [],
      "source": [
        "cost_notes = \"\"\"\n",
        "# Mistral Model Cost Notes\n",
        "\n",
        "- **Model:** mistralai/Mistral-7B-Instruct-v0.3\n",
        "- **Load in 4-bit Quantization:** Yes (nf4) — reduces VRAM cost.\n",
        "- **Runtime Environment:** Google Cloud VM with NVIDIA T4 GPU.\n",
        "- **Average Latency per Sample:** 4.443 sec\n",
        "- **Estimated Cost:** Depends on GPU rental time; with T4 on GCP preemptible instance, ~$0.3500 per hour.\n",
        "\n",
        "\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
