{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed686e06",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "OpenRouter API error 402: {\"error\":{\"message\":\"This request requires more credits, or fewer max_tokens. You requested up to 1600 tokens, but can only afford 1531. To increase, visit https://openrouter.ai/settings/credits and upgrade to a paid account\",\"code\":402,\"metadata\":{\"provider_name\":null}},\"user_id\":\"user_31aZdvvNI5alVnWWz4yzn3CHVWq\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 186\u001b[39m, in \u001b[36mcall_claude_openrouter\u001b[39m\u001b[34m(messages, model_id, temperature, max_tokens, top_p)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[43mresp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# Include server error text to help debug\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mm\\anaconda3\\Lib\\site-packages\\requests\\models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://openrouter.ai/api/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 251\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# 6) Execute & print\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     results = \u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m===== CLAUDE (OpenRouter) EVALUATION RUN =====\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    254\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTimestamp: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().isoformat(timespec=\u001b[33m'\u001b[39m\u001b[33mseconds\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 235\u001b[39m, in \u001b[36mrun_all\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    233\u001b[39m     \u001b[38;5;66;03m# B) Feedback\u001b[39;00m\n\u001b[32m    234\u001b[39m     fb_msgs = build_feedback_prompt(domain, assign_prompt, rubric_text, submission_text)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     fb_text = \u001b[43mcall_claude_openrouter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfb_msgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m     dataset_block[\u001b[33m\"\u001b[39m\u001b[33mitems\u001b[39m\u001b[33m\"\u001b[39m].append({\n\u001b[32m    238\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: idx,\n\u001b[32m    239\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdetection\u001b[39m\u001b[33m\"\u001b[39m: det_text,\n\u001b[32m    240\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfeedback\u001b[39m\u001b[33m\"\u001b[39m: fb_text\n\u001b[32m    241\u001b[39m     })\n\u001b[32m    243\u001b[39m all_outputs.append(dataset_block)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 189\u001b[39m, in \u001b[36mcall_claude_openrouter\u001b[39m\u001b[34m(messages, model_id, temperature, max_tokens, top_p)\u001b[39m\n\u001b[32m    186\u001b[39m     resp.raise_for_status()\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# Include server error text to help debug\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOpenRouter API error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    191\u001b[39m data = resp.json()\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# OpenAI-compatible shape: choices[0].message.content\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: OpenRouter API error 402: {\"error\":{\"message\":\"This request requires more credits, or fewer max_tokens. You requested up to 1600 tokens, but can only afford 1531. To increase, visit https://openrouter.ai/settings/credits and upgrade to a paid account\",\"code\":402,\"metadata\":{\"provider_name\":null}},\"user_id\":\"user_31aZdvvNI5alVnWWz4yzn3CHVWq\"}"
     ]
    }
   ],
   "source": [
    "# ==== 0) Setup (Colab-friendly) ====\n",
    "!pip -q install requests\n",
    "\n",
    "import os, json, re, textwrap, requests\n",
    "from typing import List, Dict, Any\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "\n",
    "# Provide the OpenRouter API key securely (key looks like: sk-or-v1-xxxxxxxx)\n",
    "if \"OPENROUTER_API_KEY\" not in os.environ or not os.environ[\"OPENROUTER_API_KEY\"]:\n",
    "    os.environ[\"OPENROUTER_API_KEY\"] = getpass(\"Enter your OPENROUTER_API_KEY: \")\n",
    "\n",
    "OPENROUTER_API_KEY = os.environ[\"OPENROUTER_API_KEY\"]\n",
    "\n",
    "# OpenRouter endpoint (OpenAI-compatible schema)\n",
    "OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "MODEL_ID = \"anthropic/claude-3.5-haiku\"   # change here if needed (e.g., anthropic/claude-3.5-sonnet)\n",
    "\n",
    "# -------------------------\n",
    "# 1) File paths (uploaded)\n",
    "# -------------------------\n",
    "PATHS = [\n",
    "    \"teaching.json\",\n",
    "    \"psychology.json\",\n",
    "    \"it.json\",\n",
    "    \"engineering.json\",\n",
    "    \"accounting.json\",\n",
    "]\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# -------------------------\n",
    "# 2) Rubric -> readable text\n",
    "# -------------------------\n",
    "def rubric_to_text(rubric: Dict[str, Any]) -> str:\n",
    "    \"\"\"Flatten rubric JSON to a readable block of text for the model.\"\"\"\n",
    "    lines = []\n",
    "    rid = rubric.get(\"rubric_id\", \"rubric_id_unknown\")\n",
    "    lines.append(f\"Rubric ID: {rid}\")\n",
    "    lines.append(\"Criteria:\")\n",
    "    for c in rubric.get(\"criteria\", []):\n",
    "        cid = c.get(\"criterion_id\",\"\")\n",
    "        name = c.get(\"name\",\"\")\n",
    "        desc = c.get(\"description\",\"\")\n",
    "        lines.append(f\"- {cid}: {name}\")\n",
    "        if desc:\n",
    "            lines.append(f\"  Description: {desc}\")\n",
    "        pd = c.get(\"performance_descriptors\", {})\n",
    "        if pd:\n",
    "            lines.append(\"  Performance descriptors:\")\n",
    "            for k, v in pd.items():\n",
    "                lines.append(f\"    - {k}: {v}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Base prompts (YOUR VERSION: plain text only)\n",
    "# -------------------------\n",
    "SYSTEM_PROMPT = \"You are a careful academic assistant. Be precise and give clear structured output (not JSON, not CSV, no files).\"\n",
    "\n",
    "def build_detection_prompt(submission: str, few_shots: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Academic Integrity Detector Prompt (Plain Text)\n",
    "    Expected Output:\n",
    "        Label: Human | AI | Hybrid\n",
    "        Rationale:\n",
    "        - short bullet point 1\n",
    "        - short bullet point 2\n",
    "        Flags: style_inconsistency / high_verbatim / generic_phrasing / none\n",
    "    \"\"\"\n",
    "    shot_texts = []\n",
    "    for s in few_shots:\n",
    "        shot_texts.append(\n",
    "            f'Submission: \"\"\"{s.get(\"final_submission\",\"\")}\"\"\"\\n'\n",
    "            f'Your analysis (2–4 bullet points): <analysis>\\n'\n",
    "            f'Label: {s.get(\"label_type\",\"\")}\\n'\n",
    "        )\n",
    "    examples_block = \"\\n\\n\".join(shot_texts) if shot_texts else \"/* no examples available */\"\n",
    "\n",
    "    user = f\"\"\"\n",
    "You are an AI text-source classifier for academic integrity.\n",
    "Decide whether the student submission is Human, AI, or Hybrid (AI-assisted).\n",
    "\n",
    "Guidelines:\n",
    "- Consider discourse features (specificity, subjectivity, personal context), style consistency, local/global coherence, repetitiveness, and cliché patterns.\n",
    "- Hybrid = meaningful human writing with some AI assistance, or explicit admission of mixed use.\n",
    "\n",
    "Examples:\n",
    "{examples_block}\n",
    "\n",
    "Now analyze the NEW submission and respond in plain text with the following structure:\n",
    "Label: ...\n",
    "Rationale:\n",
    "- point 1\n",
    "- point 2\n",
    "Flags: ...\n",
    "NEW submission:\n",
    "\\\"\\\"\\\"{submission}\\\"\\\"\\\"\\n\n",
    "\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "def build_feedback_prompt(domain: str, assignment_prompt: str, rubric_text: str, submission: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Rubric-Aligned Feedback Prompt (Plain Text)\n",
    "    Expected Output:\n",
    "        Overall Summary:\n",
    "        <2–4 sentence overview>\n",
    "\n",
    "        Criteria Feedback:\n",
    "        Criterion: <criterion_id>\n",
    "        Rating: Excellent | Good | Average | Needs Improvement | Poor\n",
    "        Evidence:\n",
    "        - point 1\n",
    "        - point 2\n",
    "        Improvement Tip: one concrete suggestion\n",
    "\n",
    "        Overall Rating: Excellent | Good | Average | Needs Improvement | Poor\n",
    "    \"\"\"\n",
    "    user = f\"\"\"\n",
    "You are a supportive assessor. Provide actionable feedback aligned to the rubric.\n",
    "Return plain structured text only (no JSON, no files).\n",
    "\n",
    "Sections to include:\n",
    "1) Overall Summary: 2–4 sentences on strengths and priorities.\n",
    "2) Criteria Feedback: for each rubric criterion include:\n",
    "   - Criterion\n",
    "   - Rating (excellent, good, average, needs_improvement, poor)\n",
    "   - Evidence (1–3 bullet points citing excerpts or behaviors)\n",
    "   - Improvement Tip (one concrete step)\n",
    "3) Overall Rating: Excellent | Good | Average | Needs Improvement | Poor\n",
    "\n",
    "Context:\n",
    "- Domain: {domain}\n",
    "- Assignment prompt: {assignment_prompt}\n",
    "\n",
    "Rubric (verbatim):\n",
    "{rubric_text}\n",
    "\n",
    "Student submission:\n",
    "\\\"\\\"\\\"{submission}\\\"\\\"\\\"\\n\n",
    "\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) Claude (OpenRouter) call helper\n",
    "# ------------------------------------------------\n",
    "def call_claude_openrouter(messages: List[Dict[str, str]],\n",
    "                           model_id: str = MODEL_ID,\n",
    "                           temperature: float = 0.2,\n",
    "                           max_tokens: int = 1536,\n",
    "                           top_p: float = 0.95) -> str:\n",
    "    \"\"\"\n",
    "    Calls OpenRouter's OpenAI-compatible /chat/completions endpoint.\n",
    "    Returns the assistant's text content.\n",
    "\n",
    "    Notes:\n",
    "    - OpenRouter supports 'system' role with this endpoint; we'll pass it through as-is.\n",
    "    - We keep outputs plain text (no JSON).\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        # Optional but recommended headers:\n",
    "        \"HTTP-Referer\": \"https://colab.research.google.com/\",\n",
    "        \"X-Title\": \"AAIE-PlainText-Evaluator\",\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model_id,\n",
    "        \"messages\": messages,      # [{\"role\": \"system/user/assistant\", \"content\": \"...\"}]\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        # \"stream\": False,         # non-streaming for simplicity\n",
    "    }\n",
    "\n",
    "    resp = requests.post(OPENROUTER_URL, headers=headers, data=json.dumps(payload), timeout=120)\n",
    "    try:\n",
    "        resp.raise_for_status()\n",
    "    except requests.HTTPError as e:\n",
    "        # Include server error text to help debug\n",
    "        raise RuntimeError(f\"OpenRouter API error {resp.status_code}: {resp.text}\") from e\n",
    "\n",
    "    data = resp.json()\n",
    "    # OpenAI-compatible shape: choices[0].message.content\n",
    "    try:\n",
    "        content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        if isinstance(content, list):\n",
    "            # Some providers can return a list of segments; join as text\n",
    "            content = \"\".join(seg.get(\"text\", \"\") if isinstance(seg, dict) else str(seg) for seg in content)\n",
    "        return str(content).strip()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Unexpected response format: {json.dumps(data)[:800]}\") from e\n",
    "\n",
    "# -------------------------\n",
    "# 5) Runner\n",
    "# -------------------------\n",
    "def run_all():\n",
    "    all_outputs: List[Dict[str, Any]] = []\n",
    "\n",
    "    for path in PATHS:\n",
    "        data = load_json(path)\n",
    "        domain = data.get(\"domain\", \"Unknown\")\n",
    "        assign_prompt = data.get(\"prompt\", \"\")\n",
    "        rubric = data.get(\"rubric\", {})\n",
    "        submissions = data.get(\"submissions\", [])\n",
    "\n",
    "        rubric_text = rubric_to_text(rubric)\n",
    "        few_shots = submissions[:2] if len(submissions) >= 2 else submissions[:1]\n",
    "\n",
    "        dataset_block = {\n",
    "            \"path\": path,\n",
    "            \"domain\": domain,\n",
    "            \"assignment_prompt\": assign_prompt,\n",
    "            \"n_submissions\": len(submissions),\n",
    "            \"items\": []\n",
    "        }\n",
    "\n",
    "        for idx, sub in enumerate(submissions, start=1):\n",
    "            submission_text = sub.get(\"final_submission\", \"\")\n",
    "\n",
    "            # A) Detection\n",
    "            det_msgs = build_detection_prompt(submission_text, few_shots=few_shots)\n",
    "            det_text = call_claude_openrouter(det_msgs, max_tokens=800)\n",
    "\n",
    "            # B) Feedback\n",
    "            fb_msgs = build_feedback_prompt(domain, assign_prompt, rubric_text, submission_text)\n",
    "            fb_text = call_claude_openrouter(fb_msgs, max_tokens=1600)\n",
    "\n",
    "            dataset_block[\"items\"].append({\n",
    "                \"index\": idx,\n",
    "                \"detection\": det_text,\n",
    "                \"feedback\": fb_text\n",
    "            })\n",
    "\n",
    "        all_outputs.append(dataset_block)\n",
    "\n",
    "    return all_outputs\n",
    "\n",
    "# -------------------------\n",
    "# 6) Execute & print\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_all()\n",
    "\n",
    "    print(\"\\n===== CLAUDE (OpenRouter) EVALUATION RUN =====\")\n",
    "    print(f\"Timestamp: {datetime.now().isoformat(timespec='seconds')}\")\n",
    "    for ds in results:\n",
    "        print(\"\\n--------------------------------------------\")\n",
    "        print(f\"Source : {ds['path']}\")\n",
    "        print(f\"Domain : {ds['domain']}\")\n",
    "        print(f\"Prompt : {ds['assignment_prompt']}\")\n",
    "        print(f\"#Subs  : {ds['n_submissions']}\")\n",
    "        for item in ds[\"items\"]:\n",
    "            print(f\"\\n  Submission #{item['index']}:\")\n",
    "            print(\"  --- Detection ---\")\n",
    "            print(textwrap.indent(item[\"detection\"] or \"[No output]\", \"    \"))\n",
    "            print(\"  --- Feedback ---\")\n",
    "            print(textwrap.indent(item[\"feedback\"] or \"[No output]\", \"    \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a1ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
