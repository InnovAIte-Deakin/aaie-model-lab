# Final Assessment on Phi2 Model after Evaluation Results

## 1. Poor Accuracy in AI Detection
Across all domains (Accounting, Engineering, IT, Psychology, Teaching), Phi-2 consistently showed low detection accuracy:

- **Accounting**: 30% accuracy, while human raters reported only 16% accuracy.  
- **Engineering**: 16% accuracy, confirmed by human evaluation at 16%.  
- **Psychology & Teaching**: Both around 33% accuracy.  
- **IT**: Best domain, but only 66% accuracy.  

This means Phi-2 misclassified **2/3 of texts overall**, often labeling human-written texts as AI with very high confidence (90–97%), showing **overconfidence in wrong predictions**.

---

## 2. Misclassifications
Human criteria reveal clear patterns:

- Human texts with personal perspective, anecdotes, or emotional tone were wrongly flagged as AI.  
- AI-generated academic summaries were frequently mistaken for Hybrid, due to polished style.  
- False positives (humans flagged as AI) were frequent (e.g., **3/6 in Teaching, 2/6 in IT**).  
- Across all submissions, Phi-2 failed to correctly detect **any human-written submissions**.  

These errors mean **Phi-2 cannot be trusted in educational settings**, as it unfairly penalizes human work.

---

## 3. Feedback Generation is Generic and Misaligned
- GenAI feedback ratings were often marked *excellent* across all criteria with little variation.  
- Human evaluation gave lower, more realistic ratings compared to GenAI.  
- Both evaluations showed Phi-2’s feedback often scored **3/5 for actionability**, lacking practical improvement tips.  
- Phi-2 produced only positive feedback but failed to provide **constructive criticism** that students actually need.  

---

## 4. Conclusion
- Extremely **low accuracy in AI detection** (as low as 16% in some domains).  
- **Systematic misclassifications** that unfairly penalize human writing.  
- Feedback generation is **generic, inflated, and low in actionability**, failing to support learning.  
- Strong misalignment with human evaluation, making it **unsuitable for real-world educational use**.  
