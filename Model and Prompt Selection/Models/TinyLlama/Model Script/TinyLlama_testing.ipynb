{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import json\n",
        "from typing import List, Dict, Any"
      ],
      "metadata": {
        "id": "9xG9CTKwIbdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Using device: {'GPU' if device==0 else 'CPU'}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\" if device == 0 else None,\n",
        "    torch_dtype=torch.bfloat16 if device == 0 else torch.float32,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# Llama-style padding: set pad to eos\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "chat_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        ")\n"
      ],
      "metadata": {
        "id": "x4CTeHLeIbZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"You are a careful academic assistant. Be precise and give clear structured output (not JSON, not CSV, no files).\"\n",
        "\n",
        "# --- Strict Prompt Functions ---\n",
        "def build_detection_prompt(submission: str, few_shots: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Academic Integrity Detector Prompt\n",
        "    ----------------------------------\n",
        "    Purpose:\n",
        "        Classifies student submissions as Human, AI, or Hybrid (AI-assisted).\n",
        "\n",
        "    Technique:\n",
        "        - Role-based prompting\n",
        "        - Few-shot support\n",
        "        - CoT (reasoning encouraged but hidden from output)\n",
        "        - Output in plain text\n",
        "\n",
        "    Expected Output (example format in plain text):\n",
        "        Label: Human | AI | Hybrid\n",
        "        Rationale:\n",
        "        - short bullet point 1\n",
        "        - short bullet point 2\n",
        "        Flags: style_inconsistency / high_verbatim / generic_phrasing / none\n",
        "    \"\"\"\n",
        "    shot_texts = []\n",
        "    for s in few_shots:\n",
        "        shot_texts.append(\n",
        "            f'Submission: \"\"\"{s.get(\"final_submission\",\"\")}\"\"\"\\n'\n",
        "            f'Your analysis (2–4 bullet points): <analysis>\\n'\n",
        "            f'Label: {s.get(\"label_type\",\"\")}\\n'\n",
        "        )\n",
        "    examples_block = \"\\n\\n\".join(shot_texts) if shot_texts else \"/* no examples available */\"\n",
        "    user = f\"\"\"\n",
        "You are an AI text-source classifier for academic integrity.\n",
        "Decide whether the student submission is Human, AI, or Hybrid (AI-assisted).\n",
        "Guidelines:\n",
        "- Consider discourse features (specificity, subjectivity, personal context), style consistency, local/global coherence, repetitiveness, and cliché patterns.\n",
        "- Hybrid = meaningful human writing with some AI assistance, or explicit admission of mixed use.\n",
        "Examples:\n",
        "{examples_block}\n",
        "Now analyze the NEW submission and respond in plain text with the following structure:\n",
        "Label: ...\n",
        "Rationale:\n",
        "- point 1\n",
        "- point 2\n",
        "Flags: ...\n",
        "NEW submission:\n",
        "\\\"\\\"\\\"{submission}\\\"\\\"\\\"\\n\n",
        "\"\"\"\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user},\n",
        "    ]"
      ],
      "metadata": {
        "id": "FC7SDlNmIbXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_feedback_prompt(domain: str, assignment_prompt: str, rubric_text: str, submission: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Rubric-Aligned Feedback Prompt\n",
        "    ------------------------------\n",
        "    Purpose:\n",
        "        Generates structured, supportive feedback for a student submission.\n",
        "    \"\"\"\n",
        "    user = f\"\"\"\n",
        "You are a supportive assessor. Provide actionable feedback aligned to the rubric.\n",
        "Return plain structured text only (no JSON, no files).\n",
        "Sections to include:\n",
        "1) Overall Summary: 2–4 sentences on strengths and priorities.\n",
        "2) Criteria Feedback: for each rubric criterion include:\n",
        "   - Criterion\n",
        "   - Rating (excellent, good, average, needs_improvement, poor)\n",
        "   - Evidence (1–3 bullet points citing excerpts or behaviors)\n",
        "   - Improvement Tip (one concrete step)\n",
        "3) Suggested Grade: short string (optional)\n",
        "Context:\n",
        "- Domain: {domain}\n",
        "- Assignment prompt: {assignment_prompt}\n",
        "Rubric (verbatim):\n",
        "{rubric_text}\n",
        "Student submission:\n",
        "\\\"\\\"\\\"{submission}\\\"\\\"\\\"\\n\n",
        "\"\"\"\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user},\n",
        "    ]"
      ],
      "metadata": {
        "id": "jCKqmU_DIbRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Rubric formatting helper ---\n",
        "def format_rubric(rubric):\n",
        "    formatted_rubric = f\"Rubric ID: {rubric['rubric_id']}\\n\\nCriteria:\\n\"\n",
        "    for item in rubric['criteria']:\n",
        "        formatted_rubric += f\"Criterion: {item['criterion_id']}\\nName: {item['name']}\\nDescription: {item['description']}\\nPerformance Descriptors:\\n\"\n",
        "        for key, val in item['performance_descriptors'].items():\n",
        "            formatted_rubric += f\"  - {key}: {val}\\n\"\n",
        "    return formatted_rubric"
      ],
      "metadata": {
        "id": "XyhSC2PEIbLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wScu2OXE_pSP"
      },
      "outputs": [],
      "source": [
        "# --- Load JSON data ---\n",
        "with open(\"psychology.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "rubric_text = format_rubric(data['rubric'])\n",
        "few_shots = data.get(\"few_shots\", [])\n",
        "\n",
        "# --- Loop through all submissions ---\n",
        "for i, submission in enumerate(data['submissions'], 1):\n",
        "    submission_text = submission['final_submission']\n",
        "    label_type = submission.get(\"label_type\", \"Unknown\")\n",
        "\n",
        "    # Generate feedback\n",
        "    feedback_messages = build_feedback_prompt(\n",
        "        domain=data['domain'],\n",
        "        assignment_prompt=data.get(\"prompt\", \"Analyze student submission and provide detailed feedback\"),\n",
        "        rubric_text=rubric_text,\n",
        "        submission=submission_text\n",
        "    )\n",
        "    feedback_outputs = chat_pipe(\n",
        "        feedback_messages,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    feedback_response = feedback_outputs[0]['generated_text'].strip()\n",
        "\n",
        "    # Run AI Detection\n",
        "    detection_messages = build_detection_prompt(submission_text, few_shots)\n",
        "    detection_outputs = chat_pipe(\n",
        "        detection_messages,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    detection_response = detection_outputs[0]['generated_text'].strip()\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n--- SUBMISSION {i} (Label: {label_type}) ---\")\n",
        "    print(\"\\n--- RUBRIC-ALIGNED FEEDBACK ---\\n\")\n",
        "    print(feedback_response)\n",
        "    print(\"\\n--- ACADEMIC INTEGRITY DETECTION ---\\n\")\n",
        "    print(detection_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load JSON data ---\n",
        "with open(\"accounting.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "rubric_text = format_rubric(data['rubric'])\n",
        "few_shots = data.get(\"few_shots\", [])\n",
        "\n",
        "# --- Loop through all submissions ---\n",
        "for i, submission in enumerate(data['submissions'], 1):\n",
        "    submission_text = submission['final_submission']\n",
        "    label_type = submission.get(\"label_type\", \"Unknown\")\n",
        "\n",
        "    # Generate feedback\n",
        "    feedback_messages = build_feedback_prompt(\n",
        "        domain=data['domain'],\n",
        "        assignment_prompt=data.get(\"prompt\", \"Analyze student submission and provide detailed feedback\"),\n",
        "        rubric_text=rubric_text,\n",
        "        submission=submission_text\n",
        "    )\n",
        "    feedback_outputs = chat_pipe(\n",
        "        feedback_messages,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    feedback_response = feedback_outputs[0]['generated_text'].strip()\n",
        "\n",
        "    # Run AI Detection\n",
        "    detection_messages = build_detection_prompt(submission_text, few_shots)\n",
        "    detection_outputs = chat_pipe(\n",
        "        detection_messages,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    detection_response = detection_outputs[0]['generated_text'].strip()\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n--- SUBMISSION {i} (Label: {label_type}) ---\")\n",
        "    print(\"\\n--- RUBRIC-ALIGNED FEEDBACK ---\\n\")\n",
        "    print(feedback_response)\n",
        "    print(\"\\n--- ACADEMIC INTEGRITY DETECTION ---\\n\")\n",
        "    print(detection_response)\n"
      ],
      "metadata": {
        "id": "6Pl5xwy-Lqgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load JSON data ---\n",
        "with open(\"teaching.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "rubric_text = format_rubric(data['rubric'])\n",
        "few_shots = data.get(\"few_shots\", [])\n",
        "\n",
        "# --- Loop through all submissions ---\n",
        "for i, submission in enumerate(data['submissions'], 1):\n",
        "    submission_text = submission['final_submission']\n",
        "    label_type = submission.get(\"label_type\", \"Unknown\")\n",
        "\n",
        "    # Generate feedback\n",
        "    feedback_messages = build_feedback_prompt(\n",
        "        domain=data['domain'],\n",
        "        assignment_prompt=data.get(\"prompt\", \"Analyze student submission and provide detailed feedback\"),\n",
        "        rubric_text=rubric_text,\n",
        "        submission=submission_text\n",
        "    )\n",
        "    feedback_outputs = chat_pipe(\n",
        "        feedback_messages,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    feedback_response = feedback_outputs[0]['generated_text'].strip()\n",
        "\n",
        "    # Run AI Detection\n",
        "    detection_messages = build_detection_prompt(submission_text, few_shots)\n",
        "    detection_outputs = chat_pipe(\n",
        "        detection_messages,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    detection_response = detection_outputs[0]['generated_text'].strip()\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n--- SUBMISSION {i} (Label: {label_type}) ---\")\n",
        "    print(\"\\n--- RUBRIC-ALIGNED FEEDBACK ---\\n\")\n",
        "    print(feedback_response)\n",
        "    print(\"\\n--- ACADEMIC INTEGRITY DETECTION ---\\n\")\n",
        "    print(detection_response)\n"
      ],
      "metadata": {
        "id": "p9Y4Xn6kLqs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load JSON data ---\n",
        "with open(\"engineering.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "rubric_text = format_rubric(data['rubric'])\n",
        "few_shots = data.get(\"few_shots\", [])\n",
        "\n",
        "# --- Loop through all submissions ---\n",
        "for i, submission in enumerate(data['submissions'], 1):\n",
        "    submission_text = submission['final_submission']\n",
        "    label_type = submission.get(\"label_type\", \"Unknown\")\n",
        "\n",
        "    # Generate feedback\n",
        "    feedback_messages = build_feedback_prompt(\n",
        "        domain=data['domain'],\n",
        "        assignment_prompt=data.get(\"prompt\", \"Analyze student submission and provide detailed feedback\"),\n",
        "        rubric_text=rubric_text,\n",
        "        submission=submission_text\n",
        "    )\n",
        "    feedback_outputs = chat_pipe(\n",
        "        feedback_messages,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    feedback_response = feedback_outputs[0]['generated_text'].strip()\n",
        "\n",
        "    # Run AI Detection\n",
        "    detection_messages = build_detection_prompt(submission_text, few_shots)\n",
        "    detection_outputs = chat_pipe(\n",
        "        detection_messages,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    detection_response = detection_outputs[0]['generated_text'].strip()\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n--- SUBMISSION {i} (Label: {label_type}) ---\")\n",
        "    print(\"\\n--- RUBRIC-ALIGNED FEEDBACK ---\\n\")\n",
        "    print(feedback_response)\n",
        "    print(\"\\n--- ACADEMIC INTEGRITY DETECTION ---\\n\")\n",
        "    print(detection_response)\n"
      ],
      "metadata": {
        "id": "rCcs1VTeLqvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load JSON data ---\n",
        "with open(\"it.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "rubric_text = format_rubric(data['rubric'])\n",
        "few_shots = data.get(\"few_shots\", [])\n",
        "\n",
        "# --- Loop through all submissions ---\n",
        "for i, submission in enumerate(data['submissions'], 1):\n",
        "    submission_text = submission['final_submission']\n",
        "    label_type = submission.get(\"label_type\", \"Unknown\")\n",
        "\n",
        "    # Generate feedback\n",
        "    feedback_messages = build_feedback_prompt(\n",
        "        domain=data['domain'],\n",
        "        assignment_prompt=data.get(\"prompt\", \"Analyze student submission and provide detailed feedback\"),\n",
        "        rubric_text=rubric_text,\n",
        "        submission=submission_text\n",
        "    )\n",
        "    feedback_outputs = chat_pipe(\n",
        "        feedback_messages,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    feedback_response = feedback_outputs[0]['generated_text'].strip()\n",
        "\n",
        "    # Run AI Detection\n",
        "    detection_messages = build_detection_prompt(submission_text, few_shots)\n",
        "    detection_outputs = chat_pipe(\n",
        "        detection_messages,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    detection_response = detection_outputs[0]['generated_text'].strip()\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n--- SUBMISSION {i} (Label: {label_type}) ---\")\n",
        "    print(\"\\n--- RUBRIC-ALIGNED FEEDBACK ---\\n\")\n",
        "    print(feedback_response)\n",
        "    print(\"\\n--- ACADEMIC INTEGRITY DETECTION ---\\n\")\n",
        "    print(detection_response)\n"
      ],
      "metadata": {
        "id": "D2ghnG4KLqy7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}