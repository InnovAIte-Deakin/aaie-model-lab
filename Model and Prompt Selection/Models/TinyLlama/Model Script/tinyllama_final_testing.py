# -*- coding: utf-8 -*-
"""TinyLLama_Final_Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1atSYj-e5_ri8SQR1SfqO4sClIsVO4nBK
"""

!pip install transformers accelerate sentencepiece

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import json
import torch
from typing import List, Dict, Any

MODEL_ID = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
device = 0 if torch.cuda.is_available() else -1
print(f"Using device: {'GPU' if device==0 else 'CPU'}")

# Do not force model_max_length; load as-is from config
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="auto" if device == 0 else None,
    torch_dtype=torch.bfloat16 if device == 0 else torch.float32,
    low_cpu_mem_usage=True,
)

# Llama-style padding: set pad to eos
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

chat_pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=1024,
    do_sample=True,
    temperature=0.7,
    top_p=0.95,
    top_k=50,
)

# Helper to render a chat with the official template
def render_chat(system, user):
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": user},
    ]
    return tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

SYSTEM_PROMPT = "You are a careful academic assistant. Be precise and give clear structured output (not JSON, not CSV, no files)."

def build_detection_prompt(submission: str, few_shots: list) -> str:
    shot_texts = []
    for s in few_shots:
        shot_texts.append(
            f'Submission: """{s.get("final_submission","")}"""\n'
            f'Your analysis (2–4 bullet points): <analysis>\n'
            f'Label: {s.get("label_type","")}\n'
        )
    examples_block = "\n\n".join(shot_texts) if shot_texts else "/* no examples available */"
    user_block = f"""You are an AI text-source classifier for academic integrity.
Decide whether the student submission is Human, AI, or Hybrid (AI-assisted).
Guidelines:
- Consider discourse features (specificity, subjectivity, personal context), style consistency, local/global coherence, repetitiveness, and cliché patterns.
- Hybrid = meaningful human writing with some AI assistance, or explicit admission of mixed use.
Examples:
{examples_block}
Now analyze the NEW submission and respond in plain text with the following structure:
Label: ...
Rationale:
- point 1
- point 2
Flags: ...
NEW submission:
\"\"\"{submission}\"\"\""""
    return render_chat(SYSTEM_PROMPT, user_block)

def build_feedback_prompt(domain: str, assignment_prompt: str, rubric_text: str, submission: str) -> str:
    user_block = f"""You are a supportive assessor. Provide actionable feedback aligned to the rubric.
Return plain structured text only (no JSON, no files).
Sections to include:
1) Overall Summary: 2–4 sentences on strengths and priorities.
2) Criteria Feedback: for each rubric criterion include:
   - Criterion
   - Rating (excellent, good, average, needs_improvement, poor)
   - Evidence (1–3 bullet points citing excerpts or behaviors)
   - Improvement Tip: one concrete step
3) Suggested Grade: short string (optional)
Context:
- Domain: {domain}
- Assignment prompt: {assignment_prompt}
Rubric (verbatim):
{rubric_text}
Student submission:
\"\"\"{submission}\"\"\""""
    return render_chat(SYSTEM_PROMPT, user_block)

# Rubric formatting helper
# -----------------------------
def format_rubric(rubric):
    formatted_rubric = f"Rubric ID: {rubric['rubric_id']}\n\nCriteria:\n"
    for item in rubric['criteria']:
        formatted_rubric += f"Criterion: {item['criterion_id']}\nName: {item['name']}\nDescription: {item['description']}\nPerformance Descriptors:\n"
        for key, val in item['performance_descriptors'].items():
            formatted_rubric += f"  - {key}: {val}\n"
    return formatted_rubric

# Load JSON data
# -----------------------------
with open("psychology.json") as f:
    data = json.load(f)

rubric_text = format_rubric(data['rubric'])
few_shots = data.get("few_shots", [])

# -----------------------------
# Loop through all submissions
# -----------------------------
for i, submission in enumerate(data['submissions'], 1):
    submission_text = submission['final_submission']
    label_type = submission.get("label_type", "Unknown")

    # Generate feedback
    feedback_prompt = build_feedback_prompt(
        domain=data['domain'],
        assignment_prompt=data.get("prompt", "Analyze student submission and provide detailed feedback"),
        rubric_text=rubric_text,
        submission=submission_text
    )
    feedback_outputs = chat_pipe(
        feedback_prompt,
        return_full_text=False,
        eos_token_id=tokenizer.eos_token_id
    )
    feedback_response = feedback_outputs[0]['generated_text'].strip()

    # Run AI Detection
    detection_prompt = build_detection_prompt(submission_text, few_shots)
    detection_outputs = chat_pipe(
        detection_prompt,
        return_full_text=False,
        eos_token_id=tokenizer.eos_token_id
    )
    detection_response = detection_outputs[0]['generated_text'].strip()

    # Print results
    print(f"\n--- SUBMISSION {i} (Label: {label_type}) ---")
    print("\n--- RUBRIC-ALIGNED FEEDBACK ---\n")
    print(feedback_response)
    print("\n--- ACADEMIC INTEGRITY DETECTION ---\n")
    print(detection_response)

# Load JSON data
# -----------------------------
with open("engineering.json") as f:
    data = json.load(f)

rubric_text = format_rubric(data['rubric'])
few_shots = data.get("few_shots", [])

# -----------------------------
# Loop through all submissions
# -----------------------------
for i, submission in enumerate(data['submissions'], 1):
    submission_text = submission['final_submission']
    label_type = submission.get("label_type", "Unknown")

    # Generate feedback
    feedback_prompt = build_feedback_prompt(
        domain=data['domain'],
        assignment_prompt=data.get("prompt", "Analyze student submission and provide detailed feedback"),
        rubric_text=rubric_text,
        submission=submission_text
    )
    feedback_outputs = chat_pipe(
        feedback_prompt,
        return_full_text=False,
        eos_token_id=tokenizer.eos_token_id
    )
    feedback_response = feedback_outputs[0]['generated_text'].strip()

    # Run AI Detection
    detection_prompt = build_detection_prompt(submission_text, few_shots)
    detection_outputs = chat_pipe(
        detection_prompt,
        return_full_text=False,
        eos_token_id=tokenizer.eos_token_id
    )
    detection_response = detection_outputs[0]['generated_text'].strip()

    # Print results
    print(f"\n--- SUBMISSION {i} (Label: {label_type}) ---")
    print("\n--- RUBRIC-ALIGNED FEEDBACK ---\n")
    print(feedback_response)
    print("\n--- ACADEMIC INTEGRITY DETECTION ---\n")
    print(detection_response)

# Load JSON data
# -----------------------------
with open("it.json") as f:
    data = json.load(f)

rubric_text = format_rubric(data['rubric'])
few_shots = data.get("few_shots", [])

# -----------------------------
# Loop through all submissions
# -----------------------------
for i, submission in enumerate(data['submissions'], 1):
    submission_text = submission['final_submission']
    label_type = submission.get("label_type", "Unknown")

    # Generate feedback
    feedback_prompt = build_feedback_prompt(
        domain=data['domain'],
        assignment_prompt=data.get("prompt", "Analyze student submission and provide detailed feedback"),
        rubric_text=rubric_text,
        submission=submission_text
    )
    feedback_outputs = chat_pipe(
        feedback_prompt,
        return_full_text=False,
        eos_token_id=tokenizer.eos_token_id
    )
    feedback_response = feedback_outputs[0]['generated_text'].strip()

    # Run AI Detection
    detection_prompt = build_detection_prompt(submission_text, few_shots)
    detection_outputs = chat_pipe(
        detection_prompt,
        return_full_text=False,
        eos_token_id=tokenizer.eos_token_id
    )
    detection_response = detection_outputs[0]['generated_text'].strip()

    # Print results
    print(f"\n--- SUBMISSION {i} (Label: {label_type}) ---")
    print("\n--- RUBRIC-ALIGNED FEEDBACK ---\n")
    print(feedback_response)
    print("\n--- ACADEMIC INTEGRITY DETECTION ---\n")
    print(detection_response)

# Load JSON data
# -----------------------------
with open("teaching.json") as f:
    data = json.load(f)

rubric_text = format_rubric(data['rubric'])
few_shots = data.get("few_shots", [])

# -----------------------------
# Loop through all submissions
# -----------------------------
for i, submission in enumerate(data['submissions'], 1):
    submission_text = submission['final_submission']
    label_type = submission.get("label_type", "Unknown")

    # Generate feedback
    feedback_prompt = build_feedback_prompt(
        domain=data['domain'],
        assignment_prompt=data.get("prompt", "Analyze student submission and provide detailed feedback"),
        rubric_text=rubric_text,
        submission=submission_text
    )
    feedback_outputs = chat_pipe(
        feedback_prompt,
        return_full_text=False,
        eos_token_id=tokenizer.eos_token_id
    )
    feedback_response = feedback_outputs[0]['generated_text'].strip()

    # Run AI Detection
    detection_prompt = build_detection_prompt(submission_text, few_shots)
    detection_outputs = chat_pipe(
        detection_prompt,
        return_full_text=False,
        eos_token_id=tokenizer.eos_token_id
    )
    detection_response = detection_outputs[0]['generated_text'].strip()

    # Print results
    print(f"\n--- SUBMISSION {i} (Label: {label_type}) ---")
    print("\n--- RUBRIC-ALIGNED FEEDBACK ---\n")
    print(feedback_response)
    print("\n--- ACADEMIC INTEGRITY DETECTION ---\n")
    print(detection_response)

# Load JSON data
# -----------------------------
with open("accounting.json") as f:
    data = json.load(f)

rubric_text = format_rubric(data['rubric'])
few_shots = data.get("few_shots", [])

# -----------------------------
# Loop through all submissions
# -----------------------------
for i, submission in enumerate(data['submissions'], 1):
    submission_text = submission['final_submission']
    label_type = submission.get("label_type", "Unknown")

    # Generate feedback
    feedback_prompt = build_feedback_prompt(
        domain=data['domain'],
        assignment_prompt=data.get("prompt", "Analyze student submission and provide detailed feedback"),
        rubric_text=rubric_text,
        submission=submission_text
    )
    feedback_outputs = chat_pipe(
        feedback_prompt,
        return_full_text=False,
        eos_token_id=tokenizer.eos_token_id
    )
    feedback_response = feedback_outputs[0]['generated_text'].strip()

    # Run AI Detection
    detection_prompt = build_detection_prompt(submission_text, few_shots)
    detection_outputs = chat_pipe(
        detection_prompt,
        return_full_text=False,
        eos_token_id=tokenizer.eos_token_id
    )
    detection_response = detection_outputs[0]['generated_text'].strip()

    # Print results
    print(f"\n--- SUBMISSION {i} (Label: {label_type}) ---")
    print("\n--- RUBRIC-ALIGNED FEEDBACK ---\n")
    print(feedback_response)
    print("\n--- ACADEMIC INTEGRITY DETECTION ---\n")
    print(detection_response)