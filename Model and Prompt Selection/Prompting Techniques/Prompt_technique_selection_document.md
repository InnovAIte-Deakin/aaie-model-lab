# Phase 2 Prompting Technique Selection

**Author:** Wang  
**Contributors:** Van Hieu, Qasim, Prem

## 1. Project Overview

The Artificial Assessment Intelligence for Education (AAIE) project is designed to support educators by automating key assessment tasks while ensuring transparency and reliability. The system focuses on two main functionalities:

### 1. AI Detection
- **Objective:** Classify student submissions as Human, AI-generated, or Hybrid.
- **Features:** Provides a concise, auditable rationale and a confidence score for each classification.
- **Goal:** Enable educators to quickly and accurately identify the origin of submissions without compromising transparency.

### 2. Feedback Generation
- **Objective:** Automatically generate structured, rubric-aligned, and student-friendly feedback.
- **Features:** Includes criterion-based feedback with evidence, actionable improvement tips, and an overall performance rating.
- **Goal:** Support educators in delivering consistent, high-quality feedback efficiently.

### Implementation Approach:
- Utilize a commercial AI model with prompting techniques to perform both AI detection and feedback generation.
- Focus on optimizing accuracy, clarity, tone, actionability, latency, and maintainability throughout the system.

## 2. Model Selection: Gemini 1.5-Flash

Gemini 1.5 Flash is Google's fast, lightweight multimodal model distilled from 1.5 Pro, designed for high volume, low latency workloads like chat, summarization, captioning, and document/table extraction, while supporting long context understanding across text, images, audio, video, and PDFs via the Gemini API and Vertex AI; it emphasizes speed and cost efficiency with minimal quality trade offs, making it a strong default for production apps that don't require the highest end reasoning of Pro but benefit from scalable multimodal IO and extended context handling.

- Get a Gemini API key at Google AI Studio, then copy and store it securely (env var).
- Use the key with the Gemini API/SDKs (set apiKey or x-goog-api-key) to call Gemini 1.5 Flash; quickstart snippets are in the docs.
- On Google Cloud, use Vertex AI for managed access and enterprise controls after enabling the API and billing.

## 3. Prompting Techniques

### a. Role-Based Prompting
The AI is instructed to assume a specific role (e.g., teacher, academic integrity officer), which guides its tone, style, and focus. This ensures responses match human expectations and context, producing credible and relevant outputs.

This will help the AI adopt the perspective of an educator when detecting AI-generated submissions or generating rubric-aligned feedback.

This is the most simple prompting technique so that we won't report the result and the result does not generated by the contributor.

### b. Few-Shot Learning (FSL) Prompting
The model is given a few examples of input-output pairs to learn the desired behavior from limited data. This improves consistency and reduces errors, allowing the AI to generalize effectively without retraining.

Demonstrates examples of labeled submissions or rubric-aligned feedback to guide accurate classification and feedback generation.

**Result:**
- AI detection: 0.564 (overall)
- Generative AI: 4.783 (overall rating)

### c. Chain-of-Thought (CoT) Prompting
Chain-of-Thought prompting guides the AI to reason step by step before producing a final output. Instead of giving an immediate answer, the model explicitly explains its intermediate thinking, analyzing components of the task such as patterns, logic, or rubric criteria. This approach improves accuracy, particularly for complex or ambiguous tasks, and enhances transparency, as each reasoning step can be audited by educators. In AAIE, CoT allows the system to justify why a submission is classified as Human, AI, or Hybrid, and to systematically evaluate student answers against rubric criteria before generating comprehensive feedback.

Explains why a submission is classified as Human, AI, or Hybrid, and breaks down student answers to assess each rubric criterion before producing an overall rating.

**Result:**
- AI detection: 0.86 (overall) (**Note:** However, based on the given coded and result we found that the prompt is mainly based on few shot prompting instead of CoT)
- Generative AI: 4 (overall rating)

Comparing to the ZSL result we achieve above, I think the result difference due to the given prompt of the model. The contributor of CoT method use FSL for AI detection achieve higher result due to integrate of other prompt and carefully chose of example.

### d. Structured Prompting
**Overview:** Structured prompting instructs the AI to produce outputs in a predefined format, such as JSON objects or clearly separated sections. This ensures consistency across submissions, makes outputs machine-readable for integration with dashboards or automated grading tools, and facilitates clear interpretation for educators and students. In AAIE, structured prompting is applied to both AI detection and feedback generation: classification results include fields like label, confidence_score, and rationale, while feedback is organized into sections like Criterion Feedback, Evidence, Improvement Tips, and Overall Rating, ensuring the feedback is actionable, traceable, and easy to audit. Moreover, it was the integrated of different method: role-based, CoT, few shot learning.

**Result:**
- AI detection: 0.8 (overall)
- Generative AI: 4.8 (overall rating)

**Reference:** bal, balaji (2024). *Prompt Architectures: An Overview of structured prompting strategies*. [online] Medium. Available at: https://medium.com/@balajibal/prompt-architectures-an-overview-of-structured-prompting-strategies-05b69a494956 [Accessed 14 Sep. 2025].

## Comparison Table

| Method | Description | AI Detection Score | Feedback Generation Score |
|--------|-------------|-------------------|---------------------------|
| **Role-Based Prompting** | AI assumes a specific role (e.g., teacher, academic integrity officer) to guide tone, style, and focus | NA | NA |
| **Few-Shot Learning (FSL) Prompting** | Model given examples of input-output pairs to learn desired behavior from limited data | 0.564 | 4.783 |
| **Chain-of-Thought (CoT) Prompting** | AI reasons step-by-step, explaining intermediate thinking before final output | 0.86 | 4.0 |
| **Structured Prompting** | AI produces outputs in predefined formats (JSON, separated sections) | 0.8 | 4.8 |

## 4. Proposed Prompting Technique

### a. AI Detection
Few-Shot Learning (FSL) is the optimal approach. Even though based the statistical report, the CoT achieved the highest result, but the given code refer it more likely to be FSL. The effectiveness stems from providing concrete examples that help the model learn patterns distinguishing human from AI-generated content, achieving the highest performance compared to Structured Prompting (0.8) and FSL(without carefully prompt) Prompting (0.564). As noted, the "CoT" implementation was actually FSL-based, which explains its superior performance, allowing the model to generalize from limited but well-chosen examples without requiring extensive retraining.

However, for optimal performance, a hybrid approach combining methods was introduced to use FSL with Structured Output to leverage FSL's superior detection capability (0.86 accuracy) while structuring the output in JSON format for easy integration with dashboards and automated systems, including fields like label, confidence_score, rationale.

### b. Feedback Generative AI
For Feedback Generation, Structure Prompting appears most effective with a score of 4.8 (overall rating). This approach achieved the highest score compared to FSL Prompting (4.78) and FSL (4.0), naturally aligning with educational contexts by having the AI adopt the perspective of an educator. Role-based prompting ensures tone, style, and focus match human expectations in academic settings, with the educator persona helping generate more contextually appropriate and pedagogically sound feedback.