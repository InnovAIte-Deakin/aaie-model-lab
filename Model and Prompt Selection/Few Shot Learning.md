# Few-Shot Learning: A Research Report

**Author:** Arnav Ahuja  
**Date:** August 3, 2025  

---

## Introduction

In traditional machine learning pipelines, models require access to large amounts of labelled data to learn effectively. However, in many real-world scenarios such as personalized education, rare disease diagnosis, or low-resource languages, annotated data is scarce, costly, or time-consuming to collect. Few-Shot Learning aims to address this challenge by enabling models to generalize from just a few labelled examples per class.  

Few-shot learning mimics human learning where a person can recognize new objects or concepts after seeing just a few examples. This capability has significant implications for applications in education, healthcare, NLP, and more.  

In the context of the **AAIE project**, this technique can enable rapid deployment of adaptable AI models with high data efficiency.

---

## Problem Definition

In Few-Shot Learning, we define:  

- **N-way K-shot task:** For each of the N classes, the model is given only K labelled samples (K is small, often 1 ≤ K ≤ 10).  
- The goal is to classify unseen examples from the same N classes using only the support set of size N × K.  

**For the AI Detection module of our project:**  
- Dataset: 5 academic domains/subjects (e.g., IT, Health, Psychology, etc.), each with 2 assignments.  
- Each assignment has 3 types of student submissions:  
  1. Written entirely by a human  
  2. Generated by AI  
  3. Hybrid (AI-assisted)  
- Total: **30 student submissions** (5 × 2 × 3).  
- This forms a **3-way 1-shot classification task**.

**For the Feedback Generation module of our project:**  
- One rubric for each of the 10 assignments (2 per subject).  
- Each of the 30 student submissions has rubric-aligned feedback text, totaling 30 feedback examples.  
- Model is provided with a few examples (submission prompt, student response, aligned feedback) to guide feedback generation for unseen submissions.

This setup enables the model to learn in-context how to evaluate a submission and generate feedback even with minimal training data, which is ideal for our resource-constrained setting.

---

## Categories of Few-Shot Learning

Few-shot learning methods fall into three broad categories:

### 1. Metric-Based Learning
Learns an embedding space where samples from the same class are close.
- **Prototypical Networks:** Learns a prototype (mean embedding) per class and uses distance metrics (e.g., Euclidean) for classification.  
- **Matching Networks:** Uses attention-based comparison between query and support set.  
- **Relation Networks:** Learns a deep comparator function to judge similarity.

### 2. Optimization-Based Learning
Trains the model to quickly adapt to new tasks via few gradient updates.
- **Model-Agnostic Meta-Learning (MAML):** Optimizes for a model initialization that adapts rapidly to new tasks with few steps.  
- **Reptile:** First-order approximation of MAML, faster and simpler.

### 3. Augmentation & Hallucination-Based
Generates synthetic data to simulate having more examples.
- Uses GANs, back-translation, or data transformations to expand the few-shot dataset.  
- Applicable when domain-specific data (e.g., student writing styles, rubric items) is hard to collect in bulk.

> **Note:** Our approach broadly falls into the augmentation and hallucination-based few-shot learning category, leveraging in-context prompting with LLMs to simulate learning from limited examples without model fine-tuning.

---

## Applications in AAIE Project

Within the current scope, the AAIE system analyses student-written content to:  

1. Generate contextual, rubric-aligned feedback.  
2. Detect AI-generated or AI-assisted content.  

These tasks are challenging due to:  
- Small labelled datasets per course/unit/task.  
- High linguistic and domain diversity.  
- Scarce examples for new rubric elements or feedback types.

---

## Evaluation Metrics

Few-shot learning is typically evaluated using:  
- **Accuracy** over multiple N-way K-shot episodes.  
- **Generalization** to unseen classes/tasks.  
- **Adaptation speed** (how few steps required).

**In our project, we can define additional metrics:**  
- **Rubric Match Score:** How well generated feedback aligns with rubric items.  
- **Human Evaluation Score:** Expert rating of feedback quality.  
- **AI-Detection F1 Score:** For classifying AI vs human submissions.

---

## Limitations and Challenges

- **Sensitivity to support set quality** – mislabelled or unrepresentative examples severely hurt performance.  
- **Overfitting risk** – especially in K=1 settings (an issue we might encounter in our AI Detection module).  
- **Computational cost** – meta-learning can be resource-intensive.  
- **Difficulty in NLP** – unlike images, text has greater variation and requires semantic understanding (this might be a problem in our feedback generation task for producing rubric-aligned feedback).

---

## Conclusion

Few-shot learning offers a promising solution for intelligent systems operating under data scarcity (such as our project, as the Data team will provide us with sufficient data samples not anytime sooner than Week 8 or 9).  

By enabling generalization from limited examples, this technique allows flexible, scalable, and domain-adaptive model design for feedback generation and AI-content detection. Implementing a hybrid of metric-based and meta-learning approaches, supported by embedding techniques and transformer models, can accelerate our project’s goal of enhancing academic assessment using AI.
