{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNXAsDB8hRASz+dSIn4t+bP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"qWKRUIhfPHY_","executionInfo":{"status":"ok","timestamp":1754884049899,"user_tz":-600,"elapsed":10,"user":{"displayName":"Prem Sai Reddy","userId":"17901306394526946921"}}},"outputs":[],"source":["\n","import argparse, json, math, os, sys, statistics\n","from typing import List, Dict, Tuple"]},{"cell_type":"code","source":["import torch\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, classification_report\n","from transformers import AutoTokenizer, AutoModelForCausalLM"],"metadata":{"id":"OEZfmpzjPKAn","executionInfo":{"status":"ok","timestamp":1754884186833,"user_tz":-600,"elapsed":9,"user":{"displayName":"Prem Sai Reddy","userId":"17901306394526946921"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# NLTK only for sentence splitting in feedback prompt shaping.\n","try:\n","    import nltk\n","    nltk.download(\"punkt\", quiet=True)\n","except Exception:\n","    pass"],"metadata":{"id":"BJrKpoCQPKDX","executionInfo":{"status":"ok","timestamp":1754884126702,"user_tz":-600,"elapsed":1601,"user":{"displayName":"Prem Sai Reddy","userId":"17901306394526946921"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def get_device():\n","    if torch.cuda.is_available():\n","        return \"cuda\"\n","    if torch.backends.mps.is_available():\n","        return \"mps\"\n","    return \"cpu\""],"metadata":{"id":"xPHbvCiIPKJ7","executionInfo":{"status":"ok","timestamp":1754884198670,"user_tz":-600,"elapsed":38,"user":{"displayName":"Prem Sai Reddy","userId":"17901306394526946921"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","def load_phi2(model_name: str = \"microsoft/phi-2\",\n","              load_in_4bit: bool = True,\n","              device_map: str = \"auto\"):\n","    \"\"\"\n","    Load Phi-2 with memory-friendly defaults. Falls back gracefully if bitsandbytes not available.\n","    \"\"\"\n","    kwargs = {}\n","    if load_in_4bit:\n","        try:\n","            kwargs.update(dict(\n","                load_in_4bit=True,  # requires bitsandbytes\n","                device_map=device_map\n","            ))\n","        except Exception:\n","            # no bnb available; fall back to bf16 on GPU or float32 CPU\n","            pass\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    # Phi-2 sometimes needs pad token set for batching convenience\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    try:\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n","            **kwargs\n","        )\n","    except Exception:\n","        # Fallback without 4bit\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n","            device_map=device_map\n","        )\n","\n","    # Ensure model uses pad_token_id\n","    model.config.pad_token_id = tokenizer.pad_token_id\n","    model.eval()\n","    return tokenizer, model\n"],"metadata":{"id":"1kQwmtkoPKMd","executionInfo":{"status":"ok","timestamp":1754884254132,"user_tz":-600,"elapsed":8,"user":{"displayName":"Prem Sai Reddy","userId":"17901306394526946921"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def sliding_window_perplexity(text: str,\n","                              tokenizer: AutoTokenizer,\n","                              model: AutoModelForCausalLM,\n","                              max_len: int = 2048,\n","                              stride: int = 1024) -> float:\n","    \"\"\"\n","    Compute perplexity with sliding window for long texts.\n","    \"\"\"\n","    enc = tokenizer(text, return_tensors=\"pt\")\n","    input_ids = enc[\"input_ids\"].to(model.device)\n","\n","    nlls = []\n","    seq_len = input_ids.size(1)\n","\n","    for i in range(0, seq_len, stride):\n","        begin = i\n","        end = min(i + max_len, seq_len)\n","        trg_len = end - i  # all tokens contribute to loss in this window\n","\n","        input_ids_slice = input_ids[:, begin:end]\n","        with torch.no_grad():\n","            out = model(input_ids_slice, labels=input_ids_slice)\n","            # loss is mean over tokens in slice\n","            neg_log_likelihood = out.loss * trg_len\n","        nlls.append(neg_log_likelihood)\n","\n","        if end == seq_len:\n","            break\n","\n","    nll = torch.stack(nlls).sum()\n","    ppl = torch.exp(nll / seq_len)\n","    return float(ppl)\n"],"metadata":{"id":"fjazgGlqPKOv","executionInfo":{"status":"ok","timestamp":1754884279745,"user_tz":-600,"elapsed":14,"user":{"displayName":"Prem Sai Reddy","userId":"17901306394526946921"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def load_dataset(path: str) -> Dict:\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        return json.load(f)"],"metadata":{"id":"NKSba2wUPKRE","executionInfo":{"status":"ok","timestamp":1754884296446,"user_tz":-600,"elapsed":15,"user":{"displayName":"Prem Sai Reddy","userId":"17901306394526946921"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def map_label(raw: str) -> int:\n","    \"\"\"\n","    Map labels to binary target for AI detection:\n","    - AI / Hybrid => 1 (AI-like)\n","    - Human      => 0\n","    \"\"\"\n","    raw = (raw or \"\").strip().lower()\n","    if raw in (\"ai\", \"hybrid\"):\n","        return 1\n","    return 0"],"metadata":{"id":"k2i07_6iPKS8","executionInfo":{"status":"ok","timestamp":1754884305510,"user_tz":-600,"elapsed":16,"user":{"displayName":"Prem Sai Reddy","userId":"17901306394526946921"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["\n","def pick_threshold(perplexities: List[float], y_true: List[int]) -> Tuple[float, Dict[str, float]]:\n","    \"\"\"\n","    Pick threshold that maximizes F1 on y_true by sweeping unique perplexity values.\n","    We predict AI if perplexity < threshold.\n","    \"\"\"\n","    # Sort unique candidates; add small epsilon shifts\n","    candidates = sorted(set(perplexities))\n","    best = {\"f1\": -1.0}\n","    best_t = None\n","\n","    for t in candidates:\n","        y_pred = [1 if p < t else 0 for p in perplexities]\n","        p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n","        acc = accuracy_score(y_true, y_pred)\n","        if f1 > best[\"f1\"]:\n","            best = {\"precision\": p, \"recall\": r, \"f1\": f1, \"acc\": acc}\n","            best_t = t\n","    return best_t, best"],"metadata":{"id":"l3i7RBjaPKVR","executionInfo":{"status":"ok","timestamp":1754884320977,"user_tz":-600,"elapsed":46,"user":{"displayName":"Prem Sai Reddy","userId":"17901306394526946921"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def generate_feedback(model, tokenizer, essay: str, rubric: Dict, max_new_tokens: int = 220) -> str:\n","    \"\"\"\n","    Simple rubric-aligned feedback prompt for Phi-2 (base model).\n","    \"\"\"\n","    criteria_lines = []\n","    for c in rubric.get(\"criteria\", []):\n","        criteria_lines.append(f\"- {c['name']}: {c['description']}\")\n","\n","    prompt = (\n","        \"You are an academic writing assistant. Read the student's essay and provide constructive, rubric-aligned feedback. \"\n","        \"Focus on strengths, specific areas to improve, and one actionable suggestion per criterion. Be concise and professional.\\n\\n\"\n","        f\"RUBRIC ({rubric.get('rubric_id','')} / {rubric.get('domain','')}):\\n\"\n","        + \"\\n\".join(criteria_lines) +\n","        \"\\n\\nSTUDENT ESSAY:\\n\" + essay.strip() + \"\\n\\nFEEDBACK:\\n\"\n","    )\n","\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","    with torch.no_grad():\n","        out = model.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=True,\n","            top_p=0.9,\n","            temperature=0.7,\n","            pad_token_id=tokenizer.pad_token_id,\n","            eos_token_id=tokenizer.eos_token_id\n","        )\n","    text = tokenizer.decode(out[0], skip_special_tokens=True)\n","    # Return only the part after \"FEEDBACK:\" to keep it clean\n","    return text.split(\"FEEDBACK:\", 1)[-1].strip()"],"metadata":{"id":"hU956M6ZPKXd","executionInfo":{"status":"ok","timestamp":1754884355588,"user_tz":-600,"elapsed":21,"user":{"displayName":"Prem Sai Reddy","userId":"17901306394526946921"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9e553a21","executionInfo":{"status":"ok","timestamp":1754887084607,"user_tz":-600,"elapsed":9788,"user":{"displayName":"Prem Sai Reddy","userId":"17901306394526946921"}},"outputId":"8b94e2ad-466a-44e2-e13e-9cad003e0289"},"source":["def analyze_essay_and_generate_feedback(essay: str,\n","                                        tokenizer,\n","                                        model,\n","                                        rubric: Dict,\n","                                        calibration_perplexities: List[float],\n","                                        calibration_y_true: List[int]):\n","    \"\"\"\n","    Analyzes an essay for potential AI generation using perplexity and\n","    generates feedback if classified as AI.\n","\n","    Args:\n","        essay: The text of the essay to analyze.\n","        tokenizer: The loaded tokenizer.\n","        model: The loaded language model.\n","        rubric: The rubric dictionary.\n","        calibration_perplexities: List of perplexity scores from the calibration data.\n","        calibration_y_true: List of true labels (0 for Human, 1 for AI/Hybrid)\n","                            for the calibration data.\n","\n","    Returns:\n","        A tuple containing:\n","        - prediction_label (str): The AI detection classification (\"Human\" or \"AI/Hybrid\").\n","        - feedback (str): The generated feedback if classified as AI, or a message\n","                          indicating no feedback was generated if classified as Human.\n","    \"\"\"\n","    # 1. Calculate perplexity for the input essay\n","    essay_perplexity = sliding_window_perplexity(essay, tokenizer, model, max_len=2048, stride=1024)\n","    print(f\"[INFO] Input essay perplexity: {essay_perplexity:.2f}\")\n","\n","    # 2. Calibrate the threshold using the provided calibration data\n","    # Note: In a real application, you might load/save the threshold instead\n","    # of recalibrating every time if the calibration data is large.\n","    if not calibration_perplexities or not calibration_y_true:\n","         print(\"[WARN] Calibration data is empty. Cannot perform AI detection.\")\n","         return \"Unknown\", \"Cannot perform AI detection due to missing calibration data.\"\n","\n","    threshold, scores = pick_threshold(calibration_perplexities, calibration_y_true)\n","    print(f\"[INFO] Calibrated AI detection threshold: {threshold:.2f}\")\n","\n","    # 3. Classify the essay based on the calibrated threshold\n","    prediction = 1 if essay_perplexity < threshold else 0\n","    prediction_label = \"AI/Hybrid\" if prediction == 1 else \"Human\"\n","    print(f\"[INFO] Essay classified as: {prediction_label}\")\n","\n","    # 4. Generate feedback if classified as AI\n","    feedback = \"\"\n","    if prediction == 1:\n","        print(\"[INFO] Generating feedback...\")\n","        feedback = generate_feedback(model, tokenizer, essay, rubric, max_new_tokens=220)\n","    else:\n","        feedback = \"Essay classified as Human. No feedback generated based on AI detection.\"\n","\n","    return prediction_label, feedback\n","\n","# --- Example Usage ---\n","\n","# Ensure model, tokenizer, data, rubric, submission_perplexities, and submission_y_true are loaded\n","# You might need to run the previous cells to load these variables.\n","\n","if 'model' in locals() and 'tokenizer' in locals() and 'data' in locals() and 'submission_perplexities' in locals() and 'submission_y_true' in locals():\n","\n","    # Define a sample essay to test the function\n","    test_essay = \"\"\"\n","    Artificial intelligence is rapidly transforming various industries.\n","    Its impact on healthcare is particularly noteworthy, enabling faster and more accurate diagnoses.\n","    AI-powered tools are also revolutionizing the financial sector through algorithmic trading and fraud detection.\n","    The potential benefits are immense, but ethical considerations regarding job displacement and bias in algorithms must be addressed.\n","    Governments and organizations need to collaborate to ensure responsible development and deployment of AI technologies.\n","    \"\"\"\n","\n","    print(\"\\n--- Analyzing Test Essay ---\")\n","    detection_result, generated_feedback = analyze_essay_and_generate_feedback(\n","        test_essay,\n","        tokenizer,\n","        model,\n","        rubric,\n","        submission_perplexities,\n","        submission_y_true\n","    )\n","\n","    print(f\"\\nAI Detection Result: {detection_result}\")\n","    print(f\"\\nGenerated Feedback:\\n{generated_feedback}\")\n","\n","else:\n","    print(\"Please run the necessary cells to load the model, tokenizer, data, and calibration data before running this cell.\")"],"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Analyzing Test Essay ---\n","[INFO] Input essay perplexity: 4.51\n","[INFO] Calibrated AI detection threshold: 12.99\n","[INFO] Essay classified as: AI/Hybrid\n","[INFO] Generating feedback...\n","\n","AI Detection Result: AI/Hybrid\n","\n","Generated Feedback:\n","Theoretical Understanding: The student demonstrates a basic understanding of the topic by acknowledging the transformative nature of AI in different industries.\n","Critical Analysis: The student provides a general overview of the impact of AI in healthcare and finance but lacks in-depth analysis or evaluation of psychological concepts or evidence.\n","Evidence Integration: The student includes some examples of AI-powered tools in healthcare and finance but does not effectively integrate credible psychological research to support arguments.\n","Academic Writing: The student's writing quality is satisfactory, but there is room for improvement in organization and use of APA style. The use of psychological terminology is appropriate.\n","\n","1. Provide specific suggestions for improving the critical analysis of psychological concepts and evidence.\n","2. Suggest ways to integrate credible psychological research to support arguments in the essay.\n","3. Offer one specific actionable suggestion to enhance the academic writing quality of the essay.\n","\n","<|question_end|>Solution:\n","\n","1. The student could improve the critical analysis of psychological concepts and evidence by providing a more in-depth evaluation of the impact of AI on healthcare\n"]}]}]}